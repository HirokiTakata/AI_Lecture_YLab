{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G3ayozmcbIxk"
   },
   "source": [
    "# 基盤人工知能演習 第6回\n",
    "\n",
    "※本演習資料の二次配布・再配布はお断り致します。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "loBfWn3ZnZIG"
   },
   "source": [
    "　今回の演習は以下の通りである。\n",
    "\n",
    "　**AI6.1 | MNISTを用いた多クラス分類**\n",
    "\n",
    "　**AI6.2 | 勾配消失問題とその対策**\n",
    "\n",
    "　**AI6.3 | Dropoutによる正則化**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FfviCBH0KJYJ"
   },
   "source": [
    "## AI6.0 | 事前準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oJ6BhziwDGYY"
   },
   "source": [
    "### AI6.0.1 | 手書き文字 MNIST の準備\n",
    "\n",
    "　今回は手書き文字 MNIST に対する学習を題材に、多層パーセプトロン (multi-layer perceptron) による多クラス分類 (multi-class classification) を学んでいく。MNIST は有名なデータセットで、いろんな人が機械学習（特にニューラルネットワーク）のテスト実行の題材として利用している。\n",
    "\n",
    "　PyTorchには、 `torchvision` という画像処理向けのライブラリが別途用意されており、これを使うことでMNISTデータを自動的に処理することができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z9YVvEWUWxJj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root=\".\", train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "testset  = torchvision.datasets.MNIST(root=\".\", train=False, download=True, transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yZb3g0dqXPU6"
   },
   "outputs": [],
   "source": [
    "# torch.tensor -> numpy.array\n",
    "X_train = trainset.data.numpy()\n",
    "y_train = trainset.targets.numpy()\n",
    "X_test = testset.data.numpy()\n",
    "y_test = testset.targets.numpy()\n",
    "\n",
    "# scaled from [0,255] to [0,1] for X\n",
    "X_train = X_train / 255\n",
    "X_test  = X_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HrGAyxOJEf6j"
   },
   "source": [
    "　torchvisionの `torchvision.datasets.MNIST` 関数は `torch.tensor` に自動的に変換してくれるのだが、中身のデータを目視で確認するために、一旦 `numpy.array` に変換した。\n",
    "\n",
    "　MNISTのデータは、以下のような件数のデータが含まれている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TStlGwaUaZKC"
   },
   "source": [
    "+ `X_train`: $60000 \\text{ (images)} \\times 28 \\text{ (rows, y)} \\times 28 \\text{ (columns, x)}$\n",
    "+ `y_train`: $60000 \\text{ (labels)}$\n",
    "+ `X_test`: $10000 \\text{ (images)} \\times 28 \\text{ (rows, y)} \\times 28 \\text{ (columns, x)}$\n",
    "+ `y_test`: $10000 \\text{ (labels)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sl205s1KFOtr"
   },
   "source": [
    "　実際にPythonのコードを書いて確認してみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2RbqOtJsZH35"
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Qeau6fJFV3Z"
   },
   "source": [
    "　次に、MNISTの画像について、いくつか描画してみよう。\n",
    "`i` の値を書き換えることで、いろいろな手書き文字を見ることができる。\n",
    "また、各座標には0.0から1.0までの値が記述されているが、これが入力特徴量になる。\n",
    "\n",
    "　目視してみると、基盤DS演習で用いている手書き文字よりも解像度が高いことがわかる。あちらで利用した手書き文字データは8×8だったが、MNISTは28×28なので、一辺が3倍以上増えている。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pr_UACmc3Qgc"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Index number of an instance (change this to view another instance).\n",
    "i = 1024\n",
    "\n",
    "image = X_train[i]\n",
    "label = y_train[i]\n",
    "\n",
    "print(label)\n",
    "plt.figure(figsize=(16, 16))\n",
    "sns.heatmap(image, annot=True, fmt='.1f', square=True, cmap=\"YlGnBu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WOK4mb8JKQHn"
   },
   "source": [
    "### AI6.0.2 | livelossplotのインストール\n",
    "\n",
    "　今回のMNISTは6万件ものデータが含まれているため、学習にそれなりの時間がかかる。リアルタイムで学習の状況を見る1つの方法として、livelossplotというものを利用してみる。\n",
    "\n",
    "　livelossplotは初期状態ではインストールされていないので、以下のコマンドを利用してインストールを行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uRvi94eAHBq8"
   },
   "outputs": [],
   "source": [
    "# 以下のコマンドはPythonのコマンドではなく、Google Colab固有の方法であることに注意\n",
    "# pipというPythonのパッケージ管理ツールを使ってlivelossplotをインストールしている\n",
    "!pip install livelossplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DGQhmuhxOgkF"
   },
   "source": [
    "　色々な文字列が出て来るが、最終的に **\"Successfully installed ...\"** のような記述がされていればインストールは完了である。（もし **\"Requirement already satisfied\"** で終わっていたら、既にインストールは完了していると考えてよい）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "acqGrLbSHKCH"
   },
   "source": [
    "## AI6.1 | MNISTを用いた多クラス分類\n",
    "\n",
    "　前回の演習（基盤人工知能演習 第5回）では、クラス0（×）とクラス1（○）の2クラス分類を行った。一方、**今回使うMNISTは0から9までの10種類の数字があり、それぞれを予測する**。このように、3つ以上のクラスを同時に予測することを**多クラス分類 (multi-class classification)** と呼ぶ。\n",
    "\n",
    "　2クラス分類の時には、入力 $x$ に対応する出力 $y$ はスカラ量であり、クラス1（○）である確率を出力していた。\n",
    "しかし、 $K$ クラス分類の場合は、出力値は $K$ 次元のベクトル $y=[y_1, y_2, ..., y_K]$ にして、$y_i$は入力 $x$ がクラス $i$ である確率と考える。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EeG3b2J2SxHl"
   },
   "source": [
    "### AI6.1.1 | PyTorchのためのデータ準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "neNolwBuSUZw"
   },
   "source": [
    "　それでは、実際に学習を行うために、いろいろな作業を行おう。\n",
    "\n",
    "　最初に、`X_train`, `y_train`を`X_train`, `X_valid`, `y_train`, `y_valid`に分割する（training-validation分割）。基盤データサイエンス 第3回 では交差検証法 (cross validation) を学んだが、ニューラルネットワークでは、計算時間の都合から交差検証法をしないことが多い。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nvb-YG5OYmTO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0TfKwNFhjMGf"
   },
   "outputs": [],
   "source": [
    "# trainを50000件のtrainingデータと10000件のvalidationデータに分割する\n",
    "# 分割結果が毎回同じになるように、random_state=0を付与している\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, \n",
    "                                                      train_size=50000, random_state=0)\n",
    "\n",
    "# testはそのまま利用する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NmksRwvijHRG"
   },
   "source": [
    "　続いて、前回と同様にXをtorch用のデータに変換する。`X` は先ほど見たように2次元状にデータが並んでいるが、通常のニューラルネットワークは2次元形式で与えられても処理できないため、 $28 \\times 28$ の要素がある1次元配列に変換し、それからPyTorch形式に変換する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HrYc9g--YG1S"
   },
   "outputs": [],
   "source": [
    "X_train_flatten = X_train.reshape(50000, 28*28) # 50000件の2次元画像（28×28）を50000件の1次元のデータ（784）に変換\n",
    "X_valid_flatten = X_valid.reshape(10000, 28*28)\n",
    "X_test_flatten  = X_test.reshape(10000, 28*28) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QA2GJ0IrXVgC"
   },
   "outputs": [],
   "source": [
    "X_train_torch = torch.tensor(X_train_flatten, dtype=torch.float) # dtype=torch.floatを忘れずに\n",
    "X_valid_torch = torch.tensor(X_valid_flatten, dtype=torch.float) \n",
    "X_test_torch  = torch.tensor(X_test_flatten,  dtype=torch.float) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NFTAFr_hXIVN"
   },
   "source": [
    "　次に、$y$ をPyTorch用のデータ形式にする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mH40ErHDYz4T"
   },
   "outputs": [],
   "source": [
    "y_train_torch = torch.tensor(y_train, dtype=torch.long) # dtype=torch.longを忘れずに\n",
    "y_valid_torch = torch.tensor(y_valid, dtype=torch.long) \n",
    "y_test_torch  = torch.tensor(y_test,  dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GPUnmSA3aoJd"
   },
   "source": [
    "　これで基本準備ができた。そうしたら、あとは前回と同様にdatasetとdataloaderを定義して、モデルを作って学習を行ってみるだけだ。ただし、datasetとdataloaderはtrainだけではなくvalid, testも別に用意しよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5wEy8uNcaj0X"
   },
   "outputs": [],
   "source": [
    "# XとYの組み合わせを保持してくれる便利なモノ\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train_torch, y_train_torch)\n",
    "valid_dataset = torch.utils.data.TensorDataset(X_valid_torch, y_valid_torch)\n",
    "test_dataset  = torch.utils.data.TensorDataset(X_test_torch,  y_test_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_q9TWP6rlzx3"
   },
   "outputs": [],
   "source": [
    "batch_size = 256 # 今回は256件のデータごとに学習を行う\n",
    "\n",
    "# XとYの組み合わせを(batch_size)個ずつ出力する便利なモノ\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size)\n",
    "test_loader  = torch.utils.data.DataLoader(test_dataset,  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sr0ejX6YM7ej"
   },
   "source": [
    "### AI6.1.2 | MNISTの学習の実行\n",
    "\n",
    "　それでは、ここまでに作成したDataLoaderを利用して、学習を行ってみる。ここでは、モデル `model` 、誤差関数 `loss_fn`、および最適化手法 `opt` を引数とすることで、汎用的に学習や予測を行えるコードを作成してみる。\n",
    "\n",
    "　作ろうとするコードは多少複雑になるので、これからやることを、まず箇条書きする。\n",
    "\n",
    "1. 訓練データ `train_loader` を用いてモデルのパラメータを更新する。その時に、訓練誤差（訓練データに対する誤差関数の値）と訓練データに対する予測精度を保存しておく。\n",
    "2. 検証データ `valid_loader` を用いて、現在のモデルがどの程度の汎化性能 / 汎化誤差を持つか推定する。\n",
    "3. `liveloss` を利用して、1.および2.で計算した誤差および精度を描画する。\n",
    "4. 1.から3.をepoch数だけ繰り返す。\n",
    "\n",
    "箇条書きをしたら、それに対応するコードを関数として記述してみよう。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l_3JSmSPPmpn"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "device_GPU = torch.device(\"cuda:0\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KWDvDOBxV8BV"
   },
   "outputs": [],
   "source": [
    "# 箇条書き 1. に対応\n",
    "def update_model(model, loss_fn, opt, train_loader, device):\n",
    "  train_loss = 0\n",
    "  train_correct = 0\n",
    "  train_count = len(train_loader.dataset)\n",
    "  \n",
    "  for X, y in train_loader:\n",
    "    X = X.to(device) # GPUへデータを転送\n",
    "    y = y.to(device) # GPUへデータを転送\n",
    "    y_pred = model(X) # Xからyを予測（softmaxを行う前の値が出力される）\n",
    "    \n",
    "    _, predicted = torch.max(y_pred.data, 1) # 10クラスのうち、予測確率最大のクラス番号を取得\n",
    "    train_correct += (predicted == y).sum().item() # 予測に成功した件数をカウント（accuracy計算用）\n",
    "    \n",
    "    loss = loss_fn(y_pred, y)        # ミニバッチ内の訓練誤差の 平均 を計算\n",
    "    train_loss += loss.item()*len(y) # エポック全体の訓練誤差の 合計 を計算しておく\n",
    "    \n",
    "    # 重みの更新\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "  # エポック内の訓練誤差の平均値と予測精度を計算\n",
    "  mean_train_loss = train_loss / train_count\n",
    "  train_accuracy = train_correct / train_count\n",
    "  \n",
    "  return mean_train_loss, train_accuracy    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ou4S7gEX98t"
   },
   "outputs": [],
   "source": [
    "# 箇条書き 2. に対応\n",
    "def evaluate_model(model, loss_fn, dataloader, device):\n",
    "  model.eval() # 学習を行わない時は evaluate 状態にする （補足資料※1）\n",
    "\n",
    "  valid_loss = 0\n",
    "  valid_correct = 0\n",
    "  valid_count = len(dataloader.dataset)\n",
    "\n",
    "  for X, y in dataloader:\n",
    "    X = X.to(device) # GPUへデータを転送\n",
    "    y = y.to(device) # GPUへデータを転送\n",
    "    y_pred = model(X) # Xからyを予測（softmaxを行う前の値が出力される）\n",
    "    \n",
    "    _, predicted = torch.max(y_pred.data, 1) # 10クラスのうち、予測確率最大のクラス番号を取得\n",
    "    valid_correct += (predicted == y).sum().item() # 予測に成功した件数をカウント（accuracy計算用）\n",
    "    \n",
    "    loss = loss_fn(y_pred, y)        # ミニバッチ内の訓練誤差の 平均 を計算\n",
    "    valid_loss += loss.item()*len(y) # エポック全体の訓練誤差の 合計 を計算しておく\n",
    "    \n",
    "  mean_valid_loss = valid_loss / valid_count\n",
    "  valid_accuracy = valid_correct / valid_count\n",
    "\n",
    "  model.train() # evaluate状態からtrain状態に戻しておく\n",
    "  return mean_valid_loss, valid_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LNvdeH_wZGsq"
   },
   "outputs": [],
   "source": [
    "# 箇条書き 3. および 4. に対応\n",
    "def train(model, loss_fn, opt, train_loader, valid_loader, device, epoch=50):\n",
    "  liveloss = PlotLosses() # 描画の初期化\n",
    "  for i in range(epoch):\n",
    "    train_loss, train_accuracy = update_model(model, loss_fn, opt, train_loader, device)\n",
    "    valid_loss, valid_accuracy = evaluate_model(model, loss_fn, valid_loader, device)\n",
    "  \n",
    "    # Visualize the loss and accuracy values.\n",
    "    liveloss.update({\n",
    "        'log loss': train_loss,\n",
    "        'val_log loss': valid_loss,\n",
    "        'accuracy': train_accuracy,\n",
    "        'val_accuracy': valid_accuracy,\n",
    "    })\n",
    "    liveloss.draw()  \n",
    "  print('Accuracy: {:.4f} (valid), {:.4f} (train)'.format(valid_accuracy, train_accuracy))\n",
    "  return model # 学習したモデルを返す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8BYg-89rQNJu"
   },
   "source": [
    "最後に単層パーセプトロンのモデルを作成する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q_ac5qYUQbwt"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0) # 学習結果の再現性を担保\n",
    "\n",
    "slp = torch.nn.Sequential(\n",
    "    torch.nn.Linear(28*28, 10) # 10クラス分類なので出力は10次元\n",
    ")\n",
    "slp.to(device_GPU) # deviceへモデルを転送\n",
    "\n",
    "# 誤差関数と最適化手法を準備\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(slp.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e4QyzI1gFax5"
   },
   "source": [
    "　ここで作成したモデルを**図 AI6.1**に示す。\n",
    "\n",
    "![図 AI6.1](https://i.imgur.com/WpihxBG.png)\n",
    "\n",
    "**図 AI6.1 | 単層パーセプトロン** 最後のsoftmaxは `torch.nn.CrossEntropyLoss()` の中で定義されている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BG8k4JIJK9xZ"
   },
   "source": [
    "　softmaxはsigmoid関数の多クラス分類への拡張であり、以下の式で表すことができる。\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "\\mathrm{softmax}{(z)}_k & = & \\frac{e^{z_k}}{\\sum^9_{i=0}e^{z_i}}\n",
    "\\end{eqnarray}$\n",
    "\n",
    "このsoftmaxで変換された $\\hat{y}_k$ は**確率の要件である $0 \\le \\hat{y}_k \\le 1$ かつ $\\sum_k \\hat{y}_k = 1$\n",
    "を満たす**ので、入力されたデータが0~9のどの数字っぽいか、の予測確率であると考えることができる。\n",
    "\n",
    "　また、交差エントロピー誤差 `torch.nn.CrossEntropyLoss()` についても多クラス分類の拡張が行われている。\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "H(y, \\hat{y}) & = & -\\left(\\sum^9_{k=0} y_k\\log\\hat{y}_k\\right) \\\\\n",
    "y_k & = & \\left\\{\n",
    "\\begin{array}{l}\n",
    "      0\\ \\ (y\\ne k) \\\\\n",
    "      1\\ \\ (y=k)\n",
    "    \\end{array}\n",
    "  \\right.\n",
    "\\end{eqnarray}$\n",
    "\n",
    "上記の $y_k$ のように、$y=2$ を $[0,0,1,0,0,...]^T$ とベクトルで表現したものを **one-hot vector** と呼ぶ（1か所だけ非ゼロの値になっていることを one-hot と表現している）。\n",
    "$H(y, \\hat{y})$は総和で記述されているが、one-hot vectorの特性から $H(y, \\hat{y}) = -\\log\\hat{y}_k$ で計算できる。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jqEbW3tIZzqw"
   },
   "source": [
    "　それでは、学習を行ってみよう。数分かかるが、epoch数が増えるにしたがって予測精度が変化していく様を見ると面白い。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ldszIoj-aHtN"
   },
   "outputs": [],
   "source": [
    "# 学習の実行\n",
    "trained_model = train(slp, loss_fn, optimizer, train_loader, valid_loader, device_GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h828q3pYh4JM"
   },
   "source": [
    "　最後に作成されたモデルを使って、テストデータを予測してみよう（本当は検証 (validation) データで精度最大であったようなepochの時のモデルを利用すべきなのだが、ここでは50epoch後のモデルを使ってテストデータの予測を行う）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WYh8T1n6nc7_"
   },
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = evaluate_model(slp, loss_fn, test_loader, device_GPU)\n",
    "print(test_loss)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4ZNBwh53nwNh"
   },
   "source": [
    "　どうやら、単層パーセプトロンを用いてだいたい92%程度の予測が行えるようだ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iKaZLTFnb1Tj"
   },
   "source": [
    "-----\n",
    "##### 課題 AI6.1\n",
    "\n",
    "　これまでのコードで、784次元の画像データから10クラスの確率を出力する**単層の**パーセプトロンを構築した。\n",
    "\n",
    "　今度は**多層の**パーセプトロンを構築し、予測を行ってみよう。（入力 28×28 → 512 → 256 → 10 出力）となるような多層パーセプトロン（**図 AI6.2**）を構築し、実際に学習を行うことで、50 epoch後の訓練データ、検証データに対する正解率 (accuracy) 、およびテストデータに対する正解率 (accuracy) を答えよ。\n",
    "\n",
    "![図 AI6.2](https://i.imgur.com/llSr2UW.png)\n",
    "\n",
    "**図 AI6.2 | 多層パーセプトロン** 最後のsoftmaxは `nn.CrossEntropyLoss()` の中で定義されている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QhGO4JeBc9HC"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0) # 学習結果の再現性を担保\n",
    "\n",
    "mlp = torch.nn.Sequential(\n",
    "  torch.nn.Linear(__number__, __number__), \n",
    "  torch.nn.Sigmoid(),\n",
    "  torch.nn.Linear(__number__, __number__), \n",
    "  torch.nn.Sigmoid(),\n",
    "  torch.nn.Linear(__number__, __number__), \n",
    ")\n",
    "mlp.to(device_GPU) # deviceへモデルを転送\n",
    "\n",
    "# 誤差関数と最適化手法を準備\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(mlp.parameters(), lr=0.1)\n",
    "\n",
    "# 学習の実行\n",
    "trained_model = train(mlp, loss_fn, optimizer, train_loader, valid_loader, device_GPU)\n",
    "\n",
    "# 予測の実行\n",
    "test_loss, test_accuracy = evaluate_model(mlp, loss_fn, test_loader, device_GPU)\n",
    "print(test_loss)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yr4viRXsjbcO"
   },
   "source": [
    "------\n",
    "##### 課題 AI6.2（発展、提出する必要はありません）\n",
    "\n",
    "　`update_model()` 関数と `evaluate_model()` 関数は、よくよく見てみると非常に似ている。これらを関数をまとめて、引数に応じてモデルの更新およびモデルの評価を行う `calculate_model()` 関数を作成せよ。\n",
    "\n",
    "　関数を作成したら、この関数を用いた学習を行い、結果がほとんど変わらないことを確認せよ。\n",
    "\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oUZ4PFCxNqBk"
   },
   "source": [
    "## AI6.2 | 勾配消失問題とその対策"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wbC4YsRanmaG"
   },
   "source": [
    "　ニューラルネットワークは、多層にすることで複雑な関数を表現できることを学んできた（直前の演習では、単層に比べて多層にすることで精度が向上するはずである）。\n",
    "\n",
    "　一方、層をとりあえず深くしておけば、精度が向上するとは限らない。層が深くなりすぎた場合に、誤差逆伝播が殆ど機能しなくなる、「**勾配消失問題**」という問題があるためである。\n",
    "\n",
    "　実際に、8層のモデルを利用して試してみよう。これはかなり層が深いモデルであると言える。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YBhh13MxmZdK"
   },
   "outputs": [],
   "source": [
    "###### 最終的な結果を毎回同じ値にするおまじない ######\n",
    "torch.manual_seed(0)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "  torch.nn.Linear(28*28, 512), \n",
    "  torch.nn.Sigmoid(),\n",
    "  torch.nn.Linear(512, 512), \n",
    "  torch.nn.Sigmoid(),\n",
    "  torch.nn.Linear(512, 512), \n",
    "  torch.nn.Sigmoid(),\n",
    "  torch.nn.Linear(512, 512), \n",
    "  torch.nn.Sigmoid(),\n",
    "  torch.nn.Linear(512, 512), \n",
    "  torch.nn.Sigmoid(),\n",
    "  torch.nn.Linear(512, 512), \n",
    "  torch.nn.Sigmoid(),\n",
    "  torch.nn.Linear(512, 256), \n",
    "  torch.nn.Sigmoid(),\n",
    "  torch.nn.Linear(256, 10), \n",
    ")\n",
    "model.to(device_GPU) # deviceへモデルを転送\n",
    "\n",
    "# 誤差関数と最適化手法を準備\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# 学習の実行\n",
    "trained_model = train(model, loss_fn, optimizer, train_loader, valid_loader, device_GPU)\n",
    "\n",
    "# 予測の実行\n",
    "test_loss, test_accuracy = evaluate_model(model, loss_fn, test_loader, device_GPU)\n",
    "print(test_loss)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EG4U9Nx0rN8x"
   },
   "source": [
    "　このように、学習がほとんど進まないという状況が発生する。これは、**シグモイド関数の微分値の最大値が1/4**であり、さらに**微分値がほぼ0である領域が非常に広い**ことに起因している（cf. **図 AI6.3**のように、**誤差逆伝播の計算における計算グラフの処理は掛け算の繰り返し**であることを思い出せ）。\n",
    "\n",
    "　これを回避する方法として、ReLU (Rectified Linear Unit) という活性化関数がよく用いられている。これは $\\mathrm{ReLU}(x) = \\max(0, x)$ で表される関数で、**$x$ が正であれば微分値はどこまでも1**である（**図 AI6.4**）。\n",
    "\n",
    "\n",
    "![図 AI6.3](https://i.imgur.com/VsfdLcp.png)\n",
    "\n",
    "**図 AI6.3 | 誤差逆伝播** 図 AI5.2と同一図を再掲している。$\\Sigma, \\sigma, H$ はそれぞれ総和、シグモイド関数、交差エントロピーを意味する。\n",
    "\n",
    "\n",
    "![図 AI6.4](https://i.imgur.com/vZ3edOO.png)\n",
    "\n",
    "**図 AI6.4 | Sigmoid関数とReLU関数** 関数の傾きの差を見るために、同一縮尺で描画した。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xtI_mOsXWj2C"
   },
   "source": [
    "　同じ8層のモデルについて、 `Sigmoid()` の代わりに `ReLU()` を使って学習させてみる。なお、ReLUの場合はSigmoidを使う場合に比べて少し学習率を下げておくと良いだろう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "beL0MTC6maZG"
   },
   "outputs": [],
   "source": [
    "###### 最終的な結果を毎回同じ値にするおまじない ######\n",
    "torch.manual_seed(0)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "  torch.nn.Linear(28*28, 512), \n",
    "  torch.nn.ReLU(),\n",
    "  torch.nn.Linear(512, 512), \n",
    "  torch.nn.ReLU(),\n",
    "  torch.nn.Linear(512, 512), \n",
    "  torch.nn.ReLU(),\n",
    "  torch.nn.Linear(512, 512), \n",
    "  torch.nn.ReLU(),\n",
    "  torch.nn.Linear(512, 512), \n",
    "  torch.nn.ReLU(),\n",
    "  torch.nn.Linear(512, 512), \n",
    "  torch.nn.ReLU(),\n",
    "  torch.nn.Linear(512, 256), \n",
    "  torch.nn.ReLU(),\n",
    "  torch.nn.Linear(256, 10), \n",
    ")\n",
    "model.to(device_GPU) # deviceへモデルを転送\n",
    "\n",
    "# 誤差関数と最適化手法を準備\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.02) # 学習率を1/5に下げている \n",
    "\n",
    "# 学習の実行\n",
    "trained_model = train(model, loss_fn, optimizer, train_loader, valid_loader, device_GPU)\n",
    "\n",
    "# 予測の実行\n",
    "test_loss, test_accuracy = evaluate_model(model, loss_fn, test_loader, device_GPU)\n",
    "print(test_loss)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Daz5eN_Csz1Z"
   },
   "source": [
    "　最初こそ予測精度は向上していなかったが、20epochあたりから急激に正解率が向上、適切なモデルが構築できていることがわかる。ただ、時々大きく汎化性能が悪化することがあるため、どのepochが最も汎化性能が高いかをしっかり把握しておく必要がある（**補足資料 ※2**）。 `train()` 関数の中で、最も精度が良かったモデルを記録しておくなどのコードの修正が必要だろう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YrCmA1w66DRv"
   },
   "source": [
    "-----\n",
    "##### 課題 AI6.3\n",
    "\n",
    "　`torch.nn.ReLU()` 関数の派生として、`torch.nn.PReLU()` という活性化関数が存在する。どのような活性化関数か調べて簡潔にまとめよ。\n",
    "\n",
    "　また、前述のモデルについて、全ての活性化関数を`torch.nn.PReLU()`に取り換えて学習を行い、テストデータに対する正解率 (accuracy) を答えよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tRIH_CTt6fNS"
   },
   "outputs": [],
   "source": [
    "###### 最終的な結果を毎回同じ値にするおまじない ######\n",
    "torch.manual_seed(0)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    # 課題 AI6.4: 実装せよ\n",
    ")\n",
    "\n",
    "model.to(device_GPU) # deviceへモデルを転送\n",
    "\n",
    "# 誤差関数と最適化手法を準備\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.02)\n",
    "\n",
    "# 学習の実行\n",
    "trained_model = train(model, loss_fn, optimizer, train_loader, valid_loader, device_GPU)\n",
    "\n",
    "# 予測の実行\n",
    "test_loss, test_accuracy = evaluate_model(model, loss_fn, test_loader, device_GPU)\n",
    "print(test_loss)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q3ouUdkCNEPb"
   },
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_g_PAgbcNoOf"
   },
   "source": [
    "## AI6.3 | Dropoutによる正則化\n",
    "\n",
    "　活性化関数 `ReLU()` を用いることで、層を深くすることに成功した。一般に、層を深くした方が複雑な関数を表現しやすくなる。\n",
    "一方で、複雑な関数を表現できるということは、過学習 (over-fitting) を起こしやすい、ということでもある。\n",
    "\n",
    "　基盤人工知能 第3回では過学習への対応策としては**正則化**というものが存在し、Ridge回帰を行うことで過学習をある程度防ぐことができるということを学んだ。\n",
    "今回はニューラルネットワークでよく用いられる過学習防止手法の1つであるDropoutを利用してみる。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tp6_Xxqdgl4-"
   },
   "source": [
    "　Dropoutの導入自体は非常に簡単であり、modelの**活性化関数の後に**導入すればよい。ただし、なんでもかんでも入れればよいというわけではなく、**適切に**入れる必要がある（がこれは非常に難しい）。\n",
    "今回は、何も検討せずに、3層のニューラルネットワークの全ての活性化関数の後にDropoutを導入してみよう。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YBRAsUIxvIAc"
   },
   "outputs": [],
   "source": [
    "###### 最終的な結果を毎回同じ値にするおまじない ######\n",
    "torch.manual_seed(0)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "  torch.nn.Linear(28*28, 512), \n",
    "  torch.nn.ReLU(),\n",
    "  torch.nn.Dropout(p=0.5),\n",
    "  torch.nn.Linear(512, 256), \n",
    "  torch.nn.ReLU(),\n",
    "  torch.nn.Dropout(p=0.5),\n",
    "  torch.nn.Linear(256, 10), \n",
    ")\n",
    "model.to(device_GPU) # deviceへモデルを転送\n",
    "\n",
    "# 誤差関数と最適化手法を準備\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# 学習の実行\n",
    "trained_model = train(model, loss_fn, optimizer, train_loader, valid_loader, device_GPU)\n",
    "\n",
    "# 予測の実行\n",
    "test_loss, test_accuracy = evaluate_model(model, loss_fn, test_loader, device_GPU)\n",
    "print(test_loss)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iuqk2voUOoRH"
   },
   "source": [
    "　**Dropoutを導入することで、訓練データに対する精度が抑制され、過学習が抑えられる**。一方、**Dropoutは学習速度を低下させる**ため、Dropoutを導入して学習を行う場合には、epoch数を増加させたり、学習率を上げることを検討する必要がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p11pLU4E1QBM"
   },
   "source": [
    "-------\n",
    "##### 課題 AI6.4\n",
    "\n",
    "　直前に作成したモデルからDropoutを1つ抜いたモデルを2種類構築し、Dropout層を2つ入れた場合のモデルと比較し、最良のモデルを3種類のモデルの中から答えよ。レポートには、「最良」であると考えた理由も含めて答えること。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rll6j-B2nIdA"
   },
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2z6NkAX-3IQG"
   },
   "source": [
    "# レポート提出について\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rLzUiGyLXOdo"
   },
   "source": [
    "## レポートの提出方法\n",
    "　レポートは**答案テンプレートを用い**、**1つのファイル (.doc, .docx, .pdf)** にまとめ、**学籍番号と氏名を確認の上**、**1/23 15:00 (次回 基盤人工知能演習) までに東工大ポータルのOCW-iから提出**すること。\n",
    "ファイルのアップロード後、OCW-iで「提出済」というアイコンが表示されていることを必ず確認すること。それ以外の場合は未提出扱いとなるので十分注意すること。\n",
    "また、締め切りを過ぎるとファイルの提出ができないため、時間に余裕を持って提出を行うこと。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "USqypDsXXKm9"
   },
   "source": [
    "## 答案テンプレート\n",
    "\n",
    "```\n",
    "学籍番号:\n",
    "名前:\n",
    "\n",
    "課題 AI6.1\n",
    "訓練データに対する正解率：\n",
    "検証データに対する正解率：\n",
    "テストデータに対する正解率：\n",
    "\n",
    "課題 AI6.3\n",
    "PReLUとは：\n",
    "テストデータに対する正解率：\n",
    "\n",
    "課題 AI6.4\n",
    "最良のモデル：\n",
    "理由：\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H8sqc1frYNYs"
   },
   "source": [
    "# 補足資料\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J9pBGMlJxrbI"
   },
   "source": [
    "## ※1 なぜ `evaluate()` 時には`model.eval()`が必要なのか\n",
    "　Dropoutなど、学習と予測で異なる動作をする層が存在する（Dropoutの挙動については基盤人工知能 第6回 資料参照のこと）。そのため、重みの更新を行わないようにするだけでは不十分であり、`eval()`状態にする必要がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ImsGOOlOu1L3"
   },
   "source": [
    "## ※2 `ReLU()` を用いる時の注意点\n",
    "\n",
    "　実際の実行で示したように、`ReLU()`を用いることで学習がかなり効率化されている。一方、予測結果がしばしば極端になるようで、例えば99.99%以上の極めて高確率で正例、と予測したデータが実際には負例であった時などに誤差関数の値がとても大きな値になり、一時的に予測精度が低下したり、場合によっては学習が発散してしまう。\n",
    "\n",
    "　このような現象があるため、学習が正しく進行しているかどうか適宜確認し、うまくいっていない場合には学習率を下げるなどの対応をしよう。"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AI6_MultiLayer_Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
