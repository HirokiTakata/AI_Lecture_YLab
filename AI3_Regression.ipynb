{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xr7-j3Dz2fLM"
   },
   "source": [
    "# 基盤人工知能演習 第3回\n",
    "\n",
    "※本演習資料の二次配布・再配布はお断り致します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hEdvcR40md4S"
   },
   "source": [
    "　今回の演習の内容は以下の3つである。\n",
    "\n",
    "**AI3.1 線形回帰 (Linear regression)**\n",
    "\n",
    "**AI3.2 多項式回帰 (Polynomial regression)**\n",
    "\n",
    "**AI3.3 過剰適合を防ぐ正則化（Ridge回帰）**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BEd9rJQGr9Y7"
   },
   "source": [
    "## AI3.1 | 線形回帰 (Linear regression)\n",
    "\n",
    "　まず、$y = f(x_1, x_2)$ を、既知データ $X, y$ を利用することで線形回帰で推定することをおこなってみる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fB4h3uqHNpbd"
   },
   "source": [
    "### AI3.1.1 | データセットの作成\n",
    "\n",
    "　AI3.1 で利用する仮想的なデータセットを作成する。\n",
    "ここでは、$y = 3x_1 - x_2 + 2$ という線形の関係を満たすような、ノイズが無い10件のデータ $X, y$ を作っている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HWejgLK-tDag"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # 描画用（基盤データサイエンス演習 第2回でも利用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QndsdozmtIWG"
   },
   "outputs": [],
   "source": [
    "# データセットの作成\n",
    "n_data = 10                   # 作成するデータ数\n",
    "X = np.random.rand(n_data, 2) # n_data個のxをランダムに生成\n",
    "y = 3 * X[:, 0] - X[:, 1] + 2\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TjIq8XNPO2s5"
   },
   "source": [
    "　このコードで注意する点を以下にまとめておく。NumPyの使い方にも関わるので、しっかり確認しておこう。\n",
    "* $X, y$ ともに10件のデータをまとめて表現している。\n",
    "* $X$は、各データ$x^T=[x_1, x_2]$が**横ベクトル**として、10件縦に重なっている。（講義資料を読み返そう。確かに$X$は$x^T$を縦に重ねたものになっている）\n",
    "* Pythonでは添え字は0から始まるので、10件のデータ全ての$x_1$を取得する、という操作は`X[:,0]`に対応する。\n",
    "* NumPyでは、行列`X`に対して`X[:, 0]`とすると10件のデータ全ての0列目（一番左の列、すなわち $x_1$）を取得することができる。このため、 `y = 3*X[:, 0] - X[:, 1] + 2` と記述することで10件のデータの $y$ を同時に作ることができる。\n",
    "  * `X[:, 0]`は**すべて（`:`）の行の、0列目のデータを取得する**という意味であると考えよう。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YDhTBz7PNjXX"
   },
   "source": [
    "### AI3.1.2 | NumPyを利用した線形回帰\n",
    "\n",
    "　通常、Pythonではscikit-learnという機械学習用のライブラリを用いて線形回帰などを行うのだが、その中身は本日講義で学んだ内容に基づいている。\n",
    "\n",
    "　この演習は**講義で学んだものを実際に使う**ことも1つの目的であるので、ここではscikit-learnを使う前に、**線形代数の計算に基づいて** $y = w^Tx + b = w_1x_1 + w_2x_2 + b$ の $w, b$ の推定を行ってみよう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x8-2p3ddygAT"
   },
   "source": [
    "　まず、定数項 $b$ の処理を簡単にするため、全ての要素が1である $x_3$ を`X`に追加したものを`X_aux`として定義する。これを行うことで、 $y = w^Tx = w_1x_1 + w_2x_2 + w_3x_3 = w_1x_1 + w_2x_2 + w_3$ として表現することができ、 $b$ を $w$ に含めることができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_-2GUz0ryv5H"
   },
   "outputs": [],
   "source": [
    "ones = np.ones((10,1)) \n",
    "X_aux = np.hstack([X, ones]) # x_3の作成\n",
    "print(X_aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ScesrJWZSYOG"
   },
   "source": [
    "　それでは、この $w$ を10件のデータから推定しよう。講義資料によれば、$\\hat w = (X^TX)^{-1}X^Ty$ を計算することで、最小二乗法による $w$ の推定が行えるので、これを計算してみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YEwqOGLnFB_j"
   },
   "outputs": [],
   "source": [
    "Y = y[:, np.newaxis] # 線形代数の計算なので、縦ベクトル化しておく\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zKu_slGMwSEi"
   },
   "outputs": [],
   "source": [
    "def estimate_parameters(X_train, Y_train):\n",
    "\n",
    "  XtX = np.dot(X_train.T, X_train)\n",
    "  XtXinvXt = np.dot(np.linalg.inv(XtX), X_train.T)\n",
    "  return np.dot(XtXinvXt, Y_train)\n",
    "\n",
    "w_hat = estimate_parameters(X_aux, Y)\n",
    "print(w_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wih6s7ctEVso"
   },
   "source": [
    "　$\\hat w = [3, -1, 2]^T$ という結果が得られ、正しく $y = 3x_1 - x_2 + 2$ を推定できていることがわかる。\n",
    "\n",
    "　今回はデータにノイズが存在せず、$x$と$y$の関係が線形なので、完全に予測することができている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tJRf0gB-Dizc"
   },
   "source": [
    "------\n",
    "#### 課題 AI3.1\n",
    "\n",
    "　算出された $\\hat w$ を用いて$X = [[2, 1], [3, 2]]$ に対する $\\hat y = X\\hat w$ の予測を行うことを考える。\n",
    "以下のコードの `__xxxxx__` 部を埋めて、 $\\hat y$ を算出せよ（今回は $\\hat w$ が誤差なく推定出来ているので、$\\hat y$ は $y = 3x_1 - x_2 + 2$ に一致するはずである）。 \n",
    "\n",
    "　なお、課題提出時には以下の2つを記述せよ。\n",
    "* `__xxxxx__` に何を記述したか\n",
    "* `print(y_hat)` の出力結果（Pythonが**整数値**として処理している場合は整数値を、**実数値**として処理している場合は小数点以下第1位まで答えよ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YQ3aRM2h_ZZY"
   },
   "outputs": [],
   "source": [
    "w_hat = np.array([[3, -1, 2]]).T \n",
    "new_X = np.array([[2,1], [3,2]])           # 2つのデータ[2,1]と[3,2] を予測する \n",
    "new_X = np.hstack([new_X, np.ones((2,1))]) # new_Xにもx_3を追加してあげる\n",
    "print(new_X)\n",
    "y_hat = np.__xxxxx__(new_X, w_hat)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mvbOvAzpDU3V"
   },
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tyHz-vAv_-Ow"
   },
   "source": [
    "### AI3.1.3 | scikit-learnを利用した線形回帰\n",
    "\n",
    "　次に、同じ計算をscikit-learnを用いて実行してみる。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ui2aHFAjUOQS"
   },
   "source": [
    "　scikit-learnで用意されている関数は極めて賢く作られており、定数項の推定のための変数の追加などは内部で実行される。\n",
    "\n",
    "　scikit-learnで学習を行う際には、以下の順番で計算を行う。\n",
    "1.  モデル（今回は線形回帰モデル）を定義する。\n",
    "2.   `fit()` を実行する。\n",
    "3.   必要に応じて `model.coef_` や `model.intercept_` を使って各説明変数に対する重み、および定数項を確認する。\n",
    "\n",
    "以下で実際にやってみよう。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SgOc9rRZAMR4"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression \n",
    "\n",
    "# 1. 線形回帰を行うモデルの定義\n",
    "# y = w^t x + bという数式だけ準備したような状態\n",
    "model = LinearRegression()\n",
    "\n",
    "# 2. fit()の実行、内部的にはAI3.1.2の計算を行っている\n",
    "# Xは定数項を追加せずに入力する\n",
    "# yはベクトル化せず、1次元配列として入力する\n",
    "model.fit(X, y) \n",
    "\n",
    "# 3. 重みの確認\n",
    "print(model.coef_)      # w_1, w_2 の値を確認\n",
    "print(model.intercept_) # b (w_3) の値を確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X6liPm5l9HsT"
   },
   "source": [
    "　`model.fit()` に入れる`X`, `y`はそれぞれ行列と1次元配列であることに注意してほしい。printされた結果を見てみると、（ほぼ）同じ結果が得られたことが確認できる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Navya_K3VUCv"
   },
   "source": [
    "　次に、このモデルを使って新しいデータ $(x_1, x_2) = (10, 30), (2, 1)$ を予測してみよう。新しいデータへの予測は `predict()` メソッドを利用する。\n",
    "$y = 3x_1 - x_2 + 2$ なので、この2つはそれぞれ $y = 2, 7$ となるはずである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "86QHUzbU-Kzs"
   },
   "outputs": [],
   "source": [
    "# Xと同様に複数のデータをまとめた行列を作る\n",
    "X_new = np.array([[10, 30],\n",
    "                  [2, 1]])\n",
    "model.predict(X_new) # 予測結果も1次元配列として出てくるので注意"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EBSSVcJlVmWH"
   },
   "source": [
    "　これで、NumPyで実装した場合と同様に新しいデータに対する予測を行うことができた。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EfNwrXvrcMk1"
   },
   "source": [
    "## AI3.2 | 多項式回帰 (Polynomial regression) \n",
    "　次に、$y = \\sin(x)$ を多項式 $\\hat f(x) = w_0 + \\sum_{i=1}^K w_i x^i$ で近似してみることを考える。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X1JGQ8UeWnxY"
   },
   "source": [
    "### AI3.2.1 | データセットの作成\n",
    "\n",
    "　まずは、先ほどと同様に仮想的なデータを10個、ランダムに作成する。\n",
    "ただし、今回のデータはAI3.1の線形回帰のデータとは異なり、 $y$ の観測誤差が平均 0.1 程度含まれているものとする。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HXFIt7GGQ9qa"
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # 描画用（基盤データサイエンス演習 第2回でも利用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7PF8OsO0IYD9"
   },
   "outputs": [],
   "source": [
    "# create toy dataset\n",
    "np.random.seed(7) # 毎回同じランダムデータが作成できるようにするおまじない\n",
    "\n",
    "n_data = 10\n",
    "x = 6 * np.random.rand(n_data) - 3         # -3~3の値を10個ランダムに生成\n",
    "noise = 0.1 * np.random.randn(n_data)      # ノイズ\n",
    "y = np.sin(x) + noise                      # y = sin(x) + noise を計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "epbKIUjUXQLd"
   },
   "source": [
    "　作成された **$x, y$ はどちらも1次元配列である**ことに注意しよう。作成したデータをプロットしてみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nObimhW-HaiU"
   },
   "outputs": [],
   "source": [
    "# y = sin(x) のグラフをなめらかに描画するために、\n",
    "# 0.01刻みでsinの値を計算して、\n",
    "# 折れ線グラフで表示する\n",
    "xg = np.arange(-3, 3, 0.01)\n",
    "yg = np.sin(xg)\n",
    "plt.plot(xg, yg, \"red\", label=\"ground truth\")\n",
    "\n",
    "# 作成したデータを散布図として表示する\n",
    "plt.scatter(x, y, label=\"observed data\")\n",
    "\n",
    "plt.legend(loc = \"lower right\") # 凡例の表示\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "79ERkKbRXXbg"
   },
   "source": [
    "　基本的には $y = \\sin(x)$ に従っているが、 $y$ に関する観測誤差のためにわずかに上下に値がずれている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s2oovc7aX4fx"
   },
   "source": [
    "### AI3.2.2 | scikit-learnを利用した多項式回帰\n",
    "　それでは、多項式回帰を行うことで、 $\\sin(x)$ を3次関数 $\\hat f(x) = b + w_1x^1 + w_2x^2 + w_1x^3$ で近似してみる。ここで **$[s, t, u] = [x^1, x^2, x^3]$** という3つの説明変数を考えると、**3変数の線形回帰と全く同じ式**に帰着することができる（**図 AI3.1**）。\n",
    "\n",
    "![図 AI3.1](https://i.imgur.com/zNfccQt.png)\n",
    "\n",
    "**図 AI3.1 | 線形回帰と多項式回帰の関係**  定数項は $b$ で表現している。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gUKPFP-DqeV5"
   },
   "source": [
    "　このことから、scikit-learnでは、以下の手順で3次多項式の回帰を実現する。\n",
    "\n",
    "1. `PolynomialFeatures()` クラスを利用して スカラー値 $x$ を $3$ 個の説明変数 $[x_1, x_2, x_3]^T = [x^1, x^2, x^3]^T$ に拡張する。\n",
    "2. $[x_1, x_2, x_3]^T$ を入力として、線形回帰 `LinearRegression()` を行うことで、 $[w_1, w_2, w_3]^T$ および $b$ を推定する。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dDMnl0yqBhDp"
   },
   "source": [
    "　それでは早速 `PolynomialFeatures()` を使ってみよう。\n",
    "`PolynomialFeatures()` 自体も入力として行列$X$を受け取るものになっている。AI3.2.1で作成した $x$ は行列になっていないので、行列表現に変換してから、 `PolynomialFeatures()` を利用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rgq_m4fUpx93"
   },
   "outputs": [],
   "source": [
    "print(x)  # 元々のデータは1次元配列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GxwO_spzaaAF"
   },
   "outputs": [],
   "source": [
    "X = x[:, np.newaxis] # N件の、1つの特徴量からなるデータ行列に変換する\n",
    "print(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OIb1C8qLB6_K"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "K = 3 # 3次の項まで使う\n",
    "\n",
    "# fit_transform()を行うことで変換ができる\n",
    "# LinearRegressionで定数項（x^0の部分）は勝手に考慮されるので、include_bias=Falseにしている\n",
    "# include_bias=Trueだと、x^0 = 1が各データに追加される\n",
    "X_poly = PolynomialFeatures(degree=K, include_bias=False).fit_transform(X)\n",
    "print(X_poly) # x^1, x^2, x^3の順番であることに注意"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OPX-vrHua7MB"
   },
   "source": [
    "　なお、`2.20e+01`とは、$2.20\\times 10^{1} = 22.0$ という意味である。あとは、これを使って `LinearRegression()` をすれば、多項式回帰が行えるはずである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nC646SYUC1Ku"
   },
   "source": [
    "------\n",
    "#### 課題 AI3.2\n",
    "\n",
    "　上記の特徴量を用いて3.1節と同様に線形回帰を行うことで、多項式回帰が実現される。\n",
    "AI3.1.3で実行した線形回帰のコードを参考に、`LinearRegression`モデルを用いて回帰を行い、`model.coef_`と`model.intercept_`を出力せよ。\n",
    "\n",
    "　なお、課題提出時には、`model.coef_`と`model.intercept_`の結果を、**小数点以下第4桁を四捨五入した値**をレポートに記述せよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ri0mXzZE1lh"
   },
   "source": [
    "-------\n",
    "#### 課題 AI3.3\n",
    "\n",
    "　課題 AI3.2 の結果を解釈しよう。推定された3次関数式 $\\hat f(x)$ を答えよ。ただし、課題 AI3.2 と同様に、係数は全て**小数点以下第4位を四捨五入し**、`f(x) = ax^3 + bx^2 + cx + d` の形式でレポートに記述せよ。\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qyUr_-yMDKZP"
   },
   "source": [
    "### AI3.2.3 | `make_pipeline()` を利用したモデルの構築\n",
    "\n",
    "　今回の予測は、「多項式の特徴量作成」と「線形回帰」という複数のステップを踏んだ。このような場合、いちいち途中結果を出力することなく、複数のステップを1つにまとめたモデルを作る`make_pipeline()`が利用可能である。ただし、`make_pipeline()`を利用すると重みを取り出すためにひと手間必要になるので注意が必要だ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gHENcnLvE7Wv"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "K = 3 # 3次の項まで使う\n",
    "\n",
    "# 処理の順番通りにmake_pipeline(1つめ, 2つめ, ...)と入力する。\n",
    "model_polyreg = make_pipeline(PolynomialFeatures(degree=K, include_bias=False), \n",
    "                              LinearRegression())\n",
    "\n",
    "# 複数の処理をまとめてやるのも fit() だけでよい\n",
    "X = x[:, np.newaxis] \n",
    "model_polyreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NGO74LJDcs08"
   },
   "outputs": [],
   "source": [
    "# LinearRegressionはPipelineの2番目の要素なので、\n",
    "# model_polyreg[1] として LinearRegression の中身にアクセスする\n",
    "print(model_polyreg[1].coef_)\n",
    "print(model_polyreg[1].intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PzK2i5YTd8RM"
   },
   "source": [
    "#### AI3.2.4 結果の描画とパラメータ $K$ の調整\n",
    "\n",
    "　ところで予測された $\\hat f(x)$ はどれほど $\\sin(x)$ に似ているだろうか。グラフを描画して確認してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GwpS8LJfDt5p"
   },
   "outputs": [],
   "source": [
    "# 全ての描画を行う関数\n",
    "# x, y は1次元配列\n",
    "def draw(x, y, model):\n",
    "  # 学習に利用したデータ点の散布図\n",
    "  plt.scatter(x, y, label=\"observed data\")\n",
    "\n",
    "  # sin(x)の描画\n",
    "  xg = np.arange(-3, 3, 0.01)\n",
    "  yg = np.sin(xg)\n",
    "  plt.plot(xg, yg, \"red\", label=\"ground truth\")\n",
    "\n",
    "  # 推定されたf(x)の描画\n",
    "  Xg = xg[:, np.newaxis] # 行列化\n",
    "  y_est = model.predict(Xg)\n",
    "  plt.plot(xg, y_est, \"blue\", label=\"estimation\")\n",
    "\n",
    "  # 表示処理\n",
    "  plt.legend(loc = \"lower right\") # 凡例の表示\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DhXx68DLeUoo"
   },
   "outputs": [],
   "source": [
    "# 最初に作成したデータx, yとpipelineでつないだモデルを入力する\n",
    "draw(x, y, model_polyreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RPlw5e1DFnZJ"
   },
   "source": [
    "　本来の関数が赤いグラフ、今回推定した $\\hat f(x)$ が青いグラフとなっている。かなり良い近似が得られているようだ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HoieUagEesKc"
   },
   "source": [
    "　さて、描画結果を見ることができるようになり、予測結果の良しあしを考えることができるようになったので、近似する多項式の次数 $K$ の値を変更させてみて、どのように関数形状が変化するか確認してみる。$K = 1$ から $K=9$ まで変更させながら、関数の形状のかみ合い具合を確認してみよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hTi8QsRNQEpt"
   },
   "outputs": [],
   "source": [
    "K = 6 # これが多項式の次数。1から9まで変化させてみよ\n",
    "\n",
    "model_polyreg = make_pipeline(PolynomialFeatures(degree=K, include_bias=False), \n",
    "                              LinearRegression())\n",
    "X = x[:, np.newaxis] \n",
    "model_polyreg.fit(X, y)\n",
    "draw(x, y, model_polyreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9gey4GXVP0mC"
   },
   "source": [
    "　多項式の次数 $K$ が増えるに従い観測データ (observed data) と推定された関数 (estimation) のズレが少なくなり、真の関数 (ground truth) と形状も似てくるのだが、$K=5$ あたりから推定された関数の形状が $\\sin(x)$ とかみ合わなくなり始め、 $K=6$ 以降では全く関数形状が推定できなくなってしまう。\n",
    "\n",
    "　$K=6$の場合、3次多項式の要素 $x^1, x^2, x^3$ は全て含まれており、3次多項式と同程度以上の高精度な予測が期待できるはずなのだが、$x^4, x^5, x^6$の情報を使って観測データをより詳細に、**観測誤差までも**予測しようとし、結果として元の関数の形状を推定できなくなっている。これは**過学習 (over fitting)** と呼ばれる現象であり、**データ数が少なく、説明変数が多い時**に特に発生しやすい（**補足資料 ※1**）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U166nPdJS6V-"
   },
   "source": [
    "## AI3.3 | 過学習を防ぐ正則化（Ridge回帰; Ridge regression）\n",
    "\n",
    "　AI3.2で、多項式の次元数 $K$ が非常に大きい時、多項式回帰の結果は過学習していることが分かった。\n",
    "これを避ける工夫として広く用いられているのが **Ridge回帰**である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uyFofCImhR67"
   },
   "source": [
    "### AI3.3.1 | scikit-learnの`Ridge()`を利用したRidge回帰の実施\n",
    "\n",
    "　Ridge回帰はscikit-learnに`Ridge()`として定義されているので、これを利用して過学習を抑えてみる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bFLmlCq_h2_O"
   },
   "source": [
    "　`Ridge()`には正則化項の重み `alpha` をパラメータとして与える必要がある。`alpha`の値が大きいほど、過学習を抑えることができるはずだ。\n",
    "\n",
    "　以下のコードの`alpha = 0.1`の部分を様々な値に変更させながら、推定された関数形状を確認してみよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8-9U0lJLjAl7"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "K = 6\n",
    "alpha = 0.1\n",
    "\n",
    "model_poly_ridge = make_pipeline(PolynomialFeatures(degree=K, include_bias=False), \n",
    "                                 Ridge(alpha=alpha))\n",
    "X = x[:, np.newaxis] \n",
    "model_poly_ridge.fit(X, y)\n",
    "draw(x, y, model_poly_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-_oTv58Q-Ed_"
   },
   "source": [
    "　どうだろうか。正しく機能しているだろうか。**実はこれだけではなかなかうまくいかない**。\n",
    "\n",
    "　Ridge回帰の式を再度考えてみよう。 $\\lambda |w|_2^2$ という項がモデルの複雑さを制御する正則化項 (l2 norm regularization) であった。この式から、正則化項の重み $\\lambda$ （sklearnでは`alpha`）を各重み **$w_i$ に対して均等に**効かせる。\n",
    "\n",
    "　一方、値の幅が大きい特徴量（今回の場合、$x^1$ よりも $x^6$ の方が値の幅（＝分散）が大きくなっているはずである）の重み $w$ は一般に小さくなるので、正則化項の重み $\\lambda$ (`alpha`) の効き目が弱くなってしまうのである（**補足資料 ※2**）。\n",
    "\n",
    "　このように、**値の幅が異なる説明変数に対してRidge回帰を適用する場合には、特徴量の値を平均0、分散1にそろえる標準化 (Standardization) を行う**と良い。標準化を行う場合は、`make_pipeline`の`Ridge`の前に`StandardScaler()`を導入する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dw_1zltG9TEW"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "K = 6\n",
    "alpha  = 0.1\n",
    "\n",
    "model_poly_ridge = make_pipeline(PolynomialFeatures(degree=K, include_bias=False), \n",
    "                                 StandardScaler(),\n",
    "                                 Ridge(alpha=alpha))\n",
    "model_poly_ridge.fit(X, y)\n",
    "\n",
    "draw(x, y, model_poly_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GHqTl32O-byQ"
   },
   "source": [
    "　これを行うことで、高次の項まで利用した場合でも、だいぶ本来の $y = \\sin(x)$ に近い関数形状を推定することができた。このように、Ridge回帰は過学習を抑えることができる。\n",
    "\n",
    "　ただしその一方で、この予測された関数は**3次多項式による近似に比べると関数形状の推定が僅かに悪い**ことには注意する必要がある。**予測すべき対象の関数の概形が既知**で、それに対して**適切な関数（モデル）が理論や人間の感覚から推定できる**のであれば、**それより複雑なモデルを導入しても通常良い結果はもたらさない**ということは、覚えておくと良いだろう。（**※3**）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X4BzF6qfGjCw"
   },
   "source": [
    "-----------\n",
    "#### 課題 AI3.4\n",
    "\n",
    "　上記の資料では、Ridge回帰の `alpha` を0.1としていた。この値を $\\alpha = 1$ や $\\alpha = 10$ と大きくしていった場合にestimationのグラフはどう変化するだろうか。`StandardScaler()`を含めたRidge回帰パイプラインに対して様々な$\\alpha$を適用し、簡潔に解答せよ。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Rk45udCPigt"
   },
   "source": [
    "----\n",
    "\n",
    "#### 課題 AI3.5\n",
    "\n",
    "　課題 AI3.4 で示したように、 $\\alpha$ の値は予測結果に大きな影響を与え、かつ最適な $\\alpha$ の値はその時々によって変化してしまう。\n",
    "そこで、基盤データサイエンス演習 第3回で学習した `GridSearchCV()` を用いて、**5-fold 交差検証法 (cross validation)** による $\\alpha \\in [10^{-4}, 10^{-3}, ..., 10^{0}, 10^1]$ のハイパーパラメータ探索を行い、この回帰問題における最適な $\\alpha$ の値を推定せよ。（ヒント：基盤データサイエンス演習 第3回の資料を参考にするとよい）\n",
    "\n",
    "　なお、pipelineの中のモデルに対して`GridSearchCV()`を行うのは簡単ではないため（不可能ではない）、以下のコードを参考に、Ridge回帰部分のみに対して`GridSearchCV()`を実施せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BQZwJIqw070n"
   },
   "outputs": [],
   "source": [
    "# 不完全なコード\n",
    "# 基盤データサイエンス演習 第3回の資料を参考に以下のコードを埋めよ\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "K = 6\n",
    "param_grid = _____________\n",
    "X_poly = PolynomialFeatures(degree=K, include_bias=False).fit_transform(X)\n",
    "X_poly_standardized = StandardScaler().fit_transform(X_poly)\n",
    "\n",
    "grid_search_ridge = GridSearchCV(__________)\n",
    "grid_search_ridge.fit(X_poly_standardized, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "45YT7oH0HgxO"
   },
   "source": [
    "-------------------\n",
    "\n",
    "#### 課題 AI3.6（発展、提出対象ではありません）\n",
    "\n",
    "　講義資料を参考に、NumPyを用いて**Ridge回帰**と**特徴量の標準化**を実装し、予測されたグラフがほぼ同一になることを確認せよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5wf0h27fj1zV"
   },
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "37DMY818jzZy"
   },
   "source": [
    "# レポート提出について\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rLzUiGyLXOdo"
   },
   "source": [
    "## レポートの提出方法\n",
    "\n",
    "　レポートは**答案テンプレートを用い**、**1つのファイル (.doc, .docx, .pdf)** にまとめ、**学籍番号と氏名を確認の上**、**12/26 15:00 (次回 基盤人工知能演習) までに東工大ポータルのOCW-iから提出**すること。\n",
    "ファイルのアップロード後、OCW-iで「提出済」というアイコンが表示されていることを必ず確認すること。それ以外の場合は未提出扱いとなるので十分注意すること。\n",
    "また、締め切りを過ぎるとファイルの提出ができないため、時間に余裕を持って提出を行うこと。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "USqypDsXXKm9"
   },
   "source": [
    "## 答案テンプレート\n",
    "\n",
    "```\n",
    "学籍番号:\n",
    "名前:\n",
    "\n",
    "課題 AI3.1\n",
    "__xxxxx__ = __________________\n",
    "y_hat     = __________________\n",
    "\n",
    "課題 AI3.2\n",
    "model.coef_      = __________________\n",
    "model.intercept_ = __________________\n",
    "\n",
    "課題 AI3.3\n",
    "f(x) = _____x^3 + _____x^2 + _____x + _____\n",
    "（適宜符号を書き換えよ）\n",
    "\n",
    "課題 AI3.4\n",
    "（自由記述）\n",
    "\n",
    "課題 AI3.5\n",
    "alpha = \n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xtY8qdFUOmtA"
   },
   "source": [
    "# 補足資料\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CLIOaO62HvCB"
   },
   "source": [
    "## ※1 サンプルサイズが大きければ過学習は起きにくい\n",
    "\n",
    "　今回の資料では、サンプルサイズが10件のサンプル（データセット）からsin(x)の形状を推定することを行い、6次多項式など、高次多項式を利用すると過学習が発生することを確認した。\n",
    "\n",
    "　その説明の中で、「**データ数が少なく**、説明変数が多い時に過学習 (over fitting) が発生しやすい」と記載されていたものの、データ数を増やした場合の評価を行うことなく、Ridge回帰に話が進んでしまった。そこで、補足資料として**データ数が10件ではなく1000件**であった時に、6次多項式を用いてみる。\n",
    "\n",
    "　以下のコードを実行してみると、データ数が1000件もあると、Ridge回帰を導入するまでもなく、**3次多項式よりも6次多項式の方がよい関数推定を行うことができる**。モデルの柔軟性（多項式回帰なら次元数 $K$ ）は、データ数と相談しながら決定するのが望ましそうだ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l9Pz5VdTLwTc"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9qT9HCi5Igk-"
   },
   "outputs": [],
   "source": [
    "# create toy dataset\n",
    "np.random.seed(3) # 毎回同じランダムデータが作成できるようにするおまじない\n",
    "\n",
    "n_data = 1000\n",
    "x = 6 * np.random.rand(n_data) - 3         # -3~3の値を1000個ランダムに生成\n",
    "noise = 0.1 * np.random.randn(n_data)      # ノイズ\n",
    "y = np.sin(x) + noise                      # y = sin(x) + noise を計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jkrEp7iNJKPh"
   },
   "outputs": [],
   "source": [
    "K = 6 # 6次の項まで使う\n",
    "\n",
    "# 処理の順番通りにmake_pipeline(1つめ, 2つめ, ...)と入力する。\n",
    "model_polyreg = make_pipeline(PolynomialFeatures(degree=K, include_bias=False), \n",
    "                              LinearRegression())\n",
    "\n",
    "# 複数の処理をまとめてやるのも fit() だけでよい\n",
    "X = x[:, np.newaxis] \n",
    "model_polyreg.fit(X, y)\n",
    "draw(x, y, model_polyreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kC-su9egLzbh"
   },
   "source": [
    "　実応用時には人間の行動など、関数形状が予測困難なことが多く、結局Ridge回帰などの正則化を使うことが多いが、**簡単にデータ数を増やせるのであれば、それによって予測精度を改善できることがほとんど**である（簡単にデータを取ることができないから予測したい、という欲求が生まれる訳だが…）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "64PAvS8YHuSF"
   },
   "source": [
    "## ※2 標準化が必要な理由\n",
    "\n",
    "　身長と体重から何らかの値をRidge回帰を使って予測することを考える。この時、身長を[m]で表現するか、[cm]で表現するかによって、身長に対する重み $w_h$ は、100倍変化するはずである。\n",
    "\n",
    "　一方、Ridge回帰は以下の誤差関数を最小化する。\n",
    "\n",
    "$\\begin{eqnarray}\\frac{1}{n} \\sum^n_{i=1} l(f(x_i),y_i) + \\lambda||w||^2_2\\end{eqnarray}$\n",
    "\n",
    "正則化項 $\\lambda||w||^2_2$ を見ると、$w$ が100倍大きい方が正則化のペナルティが大きくなるため、身長[m]に対する重みが厳しく制限され、身長[cm]を利用した場合と異なる予測がなされてしまう。\n",
    "\n",
    "　本質的には何も差がないはずの身長の[m]と[cm]の表現で、結果を一致させるためには、説明変数を予め同じ値の幅に整えることがよく、それが標準化という操作になっているのである。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5tu15QNVtwlu"
   },
   "source": [
    "## ※3 予測対象に適したモデルの構築\n",
    "\n",
    "　今回対象とした $\\sin(x)$ は、テイラー展開を考えると、有限次数の多項式では厳密な表現は不可能である。そのため、（十分なデータがあるという仮定のもとで）多項式近似の次数を高めれば高めるほど予測精度は高まるはずである。一方で、今回学んだように次数を高めると過学習の可能性が高くなるので、多項式近似の次数はどこかで折り合いをつける必要がある。\n",
    "\n",
    "　このような場合は、**次数を上げても予測精度がほとんど向上しない場合は、なるべく次数の低いモデルを選ぶと良い**とされる（機械学習における「オッカムの剃刀」と言われることもある）。\n",
    "\n",
    "　本来、このことは基盤データサイエンスで学んだ AIC (Akaike's information criterion) や BIC (Bayesian information criterion) を導入して定量的に議論すべき点なのだが、Ridge回帰などの正則化項が加わると議論が難しくなるため、ここでは割愛する。\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AI3_Regression",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
