{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uIu3ZAkZ2Pkm"
   },
   "source": [
    "# 基盤人工知能演習 第4回\n",
    "\n",
    "※本演習資料の二次配布・再配布はお断り致します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qa1BwOBzmsLD"
   },
   "source": [
    "　今回の演習の内容は分類予測手法の1つである「**ロジスティック回帰 (Logistic regression)** 」についてである。\n",
    "\n",
    "　**AI4.0 | データセットの作成**\n",
    "\n",
    "　**AI4.1 | scikit-learnを用いたロジスティック回帰**\n",
    "\n",
    "　**AI4.2 | ロジスティック回帰を実装する**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2g65C4nu2j7Q"
   },
   "source": [
    "## AI4.0 | データセットの作成\n",
    "\n",
    "　まずは、データセットの作成を行う。今回は、2つの説明変数 $x_1, x_2$ から、クラス0（負例, Negative）、クラス1（正例, Positive）を予測するための仮想的なデータを用いる（例えば、ロボットの2つのアームの角度を $x_1, x_2$ [rad] にしたときに、物体を掴むことができたらpositive、できなければnegative、などだろうか）。\n",
    "\n",
    "　ここでは、**$-x_1 + 2x_2$の符号が最終的な結果を決める**こと、**結果がノイズで多少変化する**ことをイメージしながら、仮想的なデータを作成している。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dw4JberG2nAe"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L1nwXMbu2v96"
   },
   "outputs": [],
   "source": [
    "# 仮想的なデータの作成\n",
    "np.random.seed(0)\n",
    "n = 40\n",
    "X_train = np.random.randn(n, 2)\n",
    "\n",
    "noise = 1.6 * np.random.randn(n)                             # 結果にノイズが含まれている\n",
    "y_train_bool = - X_train[:,0] + 2 * X_train[:,1] + noise > 0 # -x1 + 2 x2 > 0 をpositiveとして定義する\n",
    "y_train = np.where(y_train_bool, 1, -1)                      # True -> 1, False -> -1 となるように変換\n",
    "\n",
    "# 観測されたデータを散布図で示す\n",
    "plt.scatter(X_train[:,0][y_train==1],X_train[:,1][y_train==1],  \n",
    "            marker=\"o\", label=\"positives\")           # positive (y =  1) を o で表示\n",
    "plt.scatter(X_train[:,0][y_train==-1],X_train[:,1][y_train==-1], \n",
    "            marker=\"x\", label=\"negatives\")           # negative (y = -1) を x で表示\n",
    "\n",
    "# 真の境界線を表示\n",
    "# y = - x_1 + 2 x_2 = 0 が境界線になるので x_1 = 2 x_2\n",
    "plt.plot([-2, 2], [-1, 1], \"gray\", label=\"ground truth\") # (-2, -1) から (2, 1) まで直線を引く\n",
    "\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y7QkCeextlwS"
   },
   "source": [
    "　正例と負例を直線で分離することは不可能だが、大まかには左上側に正例が、右下側に負例がまとまっているようである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c_v5EJxpUmY5"
   },
   "source": [
    "## AI4.1 | scikit-learnを用いたロジスティック回帰 (Logistic Regression)\n",
    "\n",
    "　今回は、まず先にscikit-learnを用いてロジスティック回帰を実行してみる。ロジスティック回帰は、線形な分類モデルである（**回帰 (Regression) と名前がついているが、分類 (Classification) の手法であることに注意してほしい**）。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DeXDhjUwsvD4"
   },
   "source": [
    "　scikit-learnにはロジスティック回帰は`LogisticRegression()`という名前で用意されているので、学習を行ってみる。計算内容の詳細は**AI4.2**節で説明するので、とりあえず利用してみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y-N_A7gIVKg6"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 2019/12/13現在、デフォルトパラメータが変化しますよ、というFutureWarningが表示される\n",
    "lr = LogisticRegression(fit_intercept=False) \n",
    "lr.fit(X_train, y_train)         # 学習の実行\n",
    "print(lr.coef_)      # w_1, w_2 の値を出力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u77vyOPDWS7P"
   },
   "source": [
    "　$w = [-0.92, 1.46]$ という値が得られた。これは、 $-0.92x_1 + 1.46x_2 \\ge 0$ であれば正例として、そうでなければ負例として予測する、という意味である。この条件から、**ロジスティック回帰の正例と負例の境界は直線になる**ことがわかる。この境界線を先ほど示したデータセットの散布図に重ね合わせてみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zSSmFKGHLn7p"
   },
   "outputs": [],
   "source": [
    "w_hat = lr.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZkGxNt3FWxls"
   },
   "outputs": [],
   "source": [
    "# 観測されたデータを散布図で示す\n",
    "plt.scatter(X_train[:,0][y_train==1],X_train[:,1][y_train==1],  \n",
    "            marker=\"o\", label=\"positives\")           # positive (y =  1) を o で表示\n",
    "plt.scatter(X_train[:,0][y_train==-1],X_train[:,1][y_train==-1], \n",
    "            marker=\"x\", label=\"negatives\")           # negative (y = -1) を x で表示\n",
    "\n",
    "# 真の境界線を表示\n",
    "# y = - x_1 + 2 x_2 = 0 が境界線になるので x_1 = 2 x_2\n",
    "plt.plot([-2, 2], [-1, 1], \"gray\", label=\"ground truth\") # (-2, -1) から (2, 1) まで直線を引く\n",
    "\n",
    "# 予測された境界線を表示\n",
    "# 境界線は必ず直線なので、2点だけ境界点を推定して、その間を直線で結んだ線分を描画する\n",
    "# ax_1 + bx_2 = 0  <=>  x_2 = -(a x_1) / b \n",
    "xx1 = np.array([-2,2])\n",
    "xx2 = -(w_hat[0] * xx1) / w_hat[1]\n",
    "plt.plot(xx1, xx2, \"red\", label=\"estimated\") \n",
    "\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h92m82ydccts"
   },
   "source": [
    "　ロジスティック回帰では、**負例、正例に属する確率を`lr.predict_proba()`を用いて計算できる**。ここでは、$x_{new} = (2,0)$の正例、負例の確率を求めてみる。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cxNVnK8-efXc"
   },
   "outputs": [],
   "source": [
    "print(lr.predict_proba([[2,0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DowuwtQ8pRDJ"
   },
   "source": [
    "　86%の確率で負例、14%の確率で正例であるという結果が得られた（今回は負例、正例の順番に並んでいることに注意せよ）。散布図上の座標と見比べると、確かに $(2,0)$ はかなり負例側 (×側) の座標なので、予測結果は適切なようだ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JySr4kiP5X9M"
   },
   "source": [
    "------\n",
    "##### 課題 AI4.1\n",
    "\n",
    "　以下のコードで作成された新しいデータ $X_{test}$, $y_{test}$ について、以下の問いに答えよ。\n",
    "\n",
    "1. これまでのモデル`lr`を用いて、$X_{test}$から $\\hat y_{test}$ を予測せよ。\n",
    "\n",
    "2. 1.の予測結果と実際のラベル$y_{test}$を比較することで正解率 (Accuracy) を計算せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cPqsgELB5yHG"
   },
   "outputs": [],
   "source": [
    "## X_test, y_testの生成\n",
    "np.random.seed(100)\n",
    "n = 10\n",
    "omega = np.random.randn()\n",
    "noise = 1.6 * np.random.randn(n)\n",
    "X_test = np.random.randn(n, 2)\n",
    "y_test_bool = - X_test[:,0] + 2 * X_test[:,1] + noise > 0\n",
    "y_test = np.where(y_test_bool, 1, -1)  # True/Falseを1/-1に変換\n",
    "print(X_test)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ncETHtq-ygds"
   },
   "source": [
    "-----\n",
    "##### 課題 AI4.2（発展、提出の必要はありません）\n",
    "\n",
    "　**データに含まれるノイズが増えると予測は難しくなるはずである**。データ生成のコードの`noise`の`1.6`を適宜書き換え、正解率 (Accuracy) がどのように変化するか確認せよ。想定通り、予測問題の難易度は変化しているだろうか（ヒント：テストデータのサンプルサイズ $n$ を100などに増やした方が安定した結果が得られ、評価しやすくなる）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "slExw7dZfENd"
   },
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Ytz_YQ77Grc"
   },
   "source": [
    "## AI4.2 | ロジスティック回帰を実装する\n",
    "\n",
    "　AI4.1で、ひとまずロジスティック回帰が利用できるようになった。ここから先は、どのように実装されているか、内部的な理解を深めていく。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "My8C7tWd7O3H"
   },
   "source": [
    "### AI4.2.1 | 分類問題における損失関数（loss function）\n",
    "\n",
    "　まず、基盤人工知能 第3回に習った回帰問題を思い出そう。回帰問題では、各予測結果に対して、実際の値 $y$ と予測した値 $y'$ の誤差（＝損失）を小さくするように学習しようと考えていた。この時に、二乗損失という損失関数を用いることで、 $w$ について**下に凸な関数**で、**1階微分が解析的に可能であり、微分値=0となるような $w$ を解析的に求められた**ため、 $L(w)$ が最小になるような $\\hat w$ を陽に記述することが可能であった。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4fZq2M5Kx9Tv"
   },
   "source": [
    "$\\begin{eqnarray}\n",
    "L(w)   & = & \\frac{1}{2n} (y-Xw)^2 \\\\\n",
    "\\hat w & = & (X^TX)^{-1}X^Ty\n",
    "\\end{eqnarray}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6P9lspuMx6u7"
   },
   "source": [
    "\n",
    "　分類問題の「誤差」を考えると、**実際の分類 $y$ と予測した分類 $y'$ が異なる確率が減少するように学習すれば分類問題を学習できる**はずである。\n",
    "\n",
    "　これにL2正則化（前回のRidge回帰と同じ）を加えた損失関数 $L(w)$ で表現すると、以下のようになる（ **$y_i$ と $y'_i$ が同符号であれば分類が正解していて、異符号であれば分類が誤っていると判断**している）。\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "L(w) & = & \\frac{1}{n} \\sum^n_{i=1}[y_i y'_i < 0] + \\lambda w^Tw \\\\\n",
    "y'_i & = & w^T x_i \\\\\n",
    "[cond ] & = & \\left\\{\n",
    "    \\begin{array}{l}\n",
    "      1 (cond\\text{ is True}) \\\\\n",
    "      0 (cond\\text{ is False})\n",
    "    \\end{array}\n",
    "  \\right.\n",
    "\\end{eqnarray}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k_8e2i1JDNNH"
   },
   "outputs": [],
   "source": [
    "def predict(w, X):\n",
    "  return np.dot(w.T, X.T)\n",
    "def loss_func_1(w, X, y, lam):\n",
    "  n = len(y)\n",
    "  return 1/n * np.sum(y * predict(w, X) < 0 ) + lam * w.dot(w) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "70TfwFjZF26N"
   },
   "source": [
    "　しかし、講義で確認した通り、これは **$w$ について下に凸な関数ではない**ため、**微分値=0**を条件としても最適な解を得ることができない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IkzBt84EEBCM"
   },
   "outputs": [],
   "source": [
    "## w2 = 1のまま、w1を変化させてみる\n",
    "lam = 0.01\n",
    "w2 = 1\n",
    "\n",
    "loss_values = []\n",
    "w1_values = np.arange(-3,3,0.01)\n",
    "for w1 in w1_values:\n",
    "  w = np.array([w1, w2])\n",
    "  loss_values.append(loss_func_1(w, X_train, y_train, lam))\n",
    "plt.plot(w1_values, loss_values, label=\"loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"w1\")\n",
    "plt.ylabel(\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QinIbHCpHx9h"
   },
   "source": [
    "　この理想的な損失関数は最適な解を求めることができないので、代わりにロジスティック損失を用いる。\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "L(w) & = & \\frac{1}{n} \\sum^n_{i=1}\\log(1+\\exp(-y_i y'_i)) + \\lambda w^Tw \\\\\n",
    "y'_i & = & w^T x_i\n",
    "\\end{eqnarray}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bqHG8hbsJVfF"
   },
   "outputs": [],
   "source": [
    "def loss_func_2(w, X, y, lam):\n",
    "  n = len(y)\n",
    "  return 1/n * np.sum(np.log(1 + np.exp(-y * predict(w, X)))) + lam * w.dot(w) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UqqwqwqHLBnS",
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-19e6d432979e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mloss_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mw1_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw1_values\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "## w2 = 1のまま、w1を変化させてみる\n",
    "lam = 0.01\n",
    "w2 = 1\n",
    "\n",
    "loss_values = []\n",
    "w1_values = np.arange(-3,3,0.01)\n",
    "for w1 in w1_values:\n",
    "  w = np.array([w1, w2])\n",
    "  loss_values.append(loss_func_2(w, X_train, y_train, lam)) #loss_func_2を計算\n",
    "plt.plot(w1_values, loss_values, label=\"loss\")\n",
    "plt.plot(w2, loss_values, label=\"loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"w1\")\n",
    "plt.ylabel(\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IIhcLhlELPj9"
   },
   "source": [
    "　ロジスティック損失は、**下に凸な関数**になり、**解析的に $w$ に関する1階偏微分が求められる**。\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "\\frac{\\partial L(w)}{\\partial w} = \\frac{1}{n}\\sum_{i=1}^n\\frac{\\exp(-y_i w^T x_i)(-y_i x_i)}{1+\\exp(-y_i w^T x_i)} + 2\\lambda w\n",
    "\\end{eqnarray}$\n",
    "\n",
    "しかし、**$\\frac{\\partial L(w)}{\\partial w} = 0$ を $w$ について解析的に解くことができない**ため、工夫が必要である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "inrLT2JtgHAQ"
   },
   "source": [
    "--------------------\n",
    "\n",
    "##### 課題 AI4.3\n",
    "\n",
    "　`loss_func_2()`について、$w_2$ についてもグラフを描画することで、下に凸になっていることを確認せよ。また、$w_1=0$ である時、lossの値が最小になる $w_2$ の値を求めよ。小数点第2位を四捨五入して、小数点第1位まで答えよ（ヒント：`np.arange()`の範囲を狭めることで表示領域を限定し、目視で答えよ）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aDroH5EJR0i9"
   },
   "source": [
    "--------------------\n",
    "\n",
    "##### 課題 AI4.4（発展）\n",
    "\n",
    "　以下の損失関数の実装を完成させ、これまでと同様の重み$w_1$と損失値とのグラフを描画することで、この損失関数は$w_1$について下に凸になっているかどうか推定せよ。なお、この損失関数は $y_i \\cdot y_i' = 1 $ なる $i$ が存在するとき一般的な微分は不可能であることに注意せよ。\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "L(w) & = & \\frac{1}{n} \\sum^n_{i=1} l(y_i, y'_i) + \\lambda w^Tw \\\\\n",
    "y'_i & = & w^T x_i \\\\\n",
    "l(y, y') & = & \\left\\{\n",
    "    \\begin{array}{l}\n",
    "      -y \\cdot y' + 1 \\text{ (}y \\cdot y' < 1\\text{)} \\\\\n",
    "      0 \\text{ (}y \\cdot y' \\ge 1\\text{)}\n",
    "    \\end{array}\n",
    "  \\right.\n",
    "\\end{eqnarray}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ymm8yOp0PE9k"
   },
   "outputs": [],
   "source": [
    "###### l()を実装せよ ######\n",
    "# y_true, y_predはともにnp.arrayであることに注意\n",
    "def l(y_true, y_pred):\n",
    "  return None\n",
    "###########################\n",
    "\n",
    "def predict(w, X):\n",
    "  return np.dot(w.T, X.T)\n",
    "def loss_func_exercise(w, X, y, lam):\n",
    "  n = len(y)\n",
    "  return 1/n * np.sum(l(y, predict(w, X))) + lam * w.dot(w) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zo9Q5--E5Hxu"
   },
   "outputs": [],
   "source": [
    "## w2 = 1のまま、w1を変化させてみる\n",
    "lam = 0.01\n",
    "w2 = 1\n",
    "\n",
    "loss_values = []\n",
    "w1_values = np.arange(-1,0,0.01)\n",
    "for w1 in w1_values:\n",
    "  w = np.array([w1, w2])\n",
    "  loss_values.append(loss_func_exercise(w, X_train, y_train, lam)) \n",
    "plt.plot(w1_values, loss_values, label=\"loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"w1\")\n",
    "plt.ylabel(\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jc4gJmsf9K9Q"
   },
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qTkzx0fxRTh3"
   },
   "source": [
    "### AI4.2.2 | 最急降下法によるパラメータの最適化\n",
    "\n",
    "　ロジスティック損失によって、下に凸な損失関数を定義することはできたが、この関数は損失関数の値が最小になるような $w$ の値を解析的に求めることはできなかった。\n",
    "しかし、コンピュータを用いた計算の場合、求めたい値が解析的には求められない場合でも、**繰り返しの計算を行うことで、数値的に答えに近づくことが可能**である。\n",
    "この節では、最も単純な**最急降下法 (steepest gradient descent)** を用いた $w$ の最適化を行う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m7Ka4obey8S8"
   },
   "source": [
    "#### 最急降下法のイメージ\n",
    "\n",
    "　まず、最急降下法のイメージを掴むために、$y=x^2$ について、 $y$ が最小になる $x$ を求めることを考えよう。\n",
    "もし、関数が下に凸であれば、**関数の坂に玉を転がせば、いつかは $y$ の最小値にたどりつける**はずである（**図 AI4.1**）。これは、**坂を下る方向に最適な $x$ が存在する**という仮定を置いていることになる。\n",
    "\n",
    "　最急降下法では、$\\Delta t$ 時間後の玉の座標を、元いた座標における $y$ の勾配 $\\left.\\frac{dy}{dx}\\right|_{x=x_0}$から推定する。\n",
    "\n",
    "![図 AI4.1](https://i.imgur.com/tVHnJNE.png)\n",
    "\n",
    "**図 AI4.1 | 最急降下法のイメージ**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "drmEPN5hjNs1"
   },
   "source": [
    "#### 数値計算による最適化の枠組み\n",
    "\n",
    "　数値的に最適な$w$を計算するためには、（仮想的な）時刻$t$を定め、以下のような更新式を繰り返し実行する。\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "w^{(t+1)} & = & w^{(t)} + \\alpha^{(t)}d^{(t)}\n",
    "\\end{eqnarray}$\n",
    "\n",
    "この式において、$d^{(t)}$は最適な $w$ が存在すると考えられる方向（ベクトル）であり、$\\alpha^{(t)}$は時刻$t$における更新量（$\\Delta t$ に相当）を定めている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p6GVc-eNmX-N"
   },
   "outputs": [],
   "source": [
    "def optimize(X, y, lam, alpha, niter=1000):\n",
    "  w = np.zeros(2) # 時刻t=0におけるwは (0,0) とする\n",
    "  ww = [w]        # 後の描画のために履歴を残す\n",
    "  \n",
    "  for t in range(niter):\n",
    "    w = w + alpha * direction(X, y, w, lam) # update (alphaは固定としている)\n",
    "    ww.append(w)\n",
    "\n",
    "  return np.array(ww) # 履歴を全て出力する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HHmHn5PFngdo"
   },
   "source": [
    "#### 最急降下法における更新幅 $\\alpha^{(t)}$ と探索方向 $d^{(t)}$ \n",
    "\n",
    "　各最適化手法で異なるのは $\\alpha^{(t)}$ と $d^{(t)}$ である。最急降下法においては $\\alpha$ は $t$ によらず一定とすることが多いので、ここではそれに従う。\n",
    "\n",
    "　最急降下法では、**関数の形を坂のようなものであると考え、ある点の勾配 (gradient) を計算して、坂を下る方向に最適解 $w$ がある**という仮説に基づき、 $d^{(t)}$ を決定する。\n",
    "勾配とは、点$w^{(t)}$における最適化したい関数 $f(w)$（今回の場合は損失関数$L(w)$）の $w$ による1階偏微分 $\\frac{\\partial L(w)}{\\partial w}$ に相当する。\n",
    "\n",
    "　先ほども記述したが、ロジスティック損失 $L(w)$ は1階微分が可能なので、解析的に座標 $w$ における微分値を計算することが可能である（**補足資料 ※1**）。\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "\\frac{\\partial L(w)}{\\partial w} = \\frac{1}{n}\\sum_{i=1}^n\\frac{\\exp(-y_i w^T x_i)(-y_i x_i)}{1+\\exp(-y_i w^T x_i)} + 2\\lambda w\n",
    "\\end{eqnarray}$\n",
    "\n",
    "ただし、値の最小化を行うことから、勾配の負の方向に進むようにしよう。すなわち、 $d^{(t)} = -\\frac{\\partial L(w)}{\\partial w}$ となる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yv-sfkRSsBUP"
   },
   "outputs": [],
   "source": [
    "def predict(w, X):\n",
    "  return np.dot(w.T, X.T)\n",
    "\n",
    "def direction(X, y, w, lam):\n",
    "  block = np.exp(-y * predict(w, X))\n",
    "  numerator = block[:,np.newaxis] * (-y[:,np.newaxis] * X)\n",
    "  denominator = 1 + block\n",
    "  delta_w = 1 / len(y) * np.sum(numerator / denominator[:,np.newaxis], axis=0) + 2 * lam * w\n",
    "  return - delta_w # 負の方向"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QIbtip7LsCZl"
   },
   "source": [
    "　これで先ほど実装したoptimizeが動くはずだ。実行してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HXqw64tt8-XU"
   },
   "outputs": [],
   "source": [
    "## gradient discent\n",
    "lam = 0.0125\n",
    "alpha = 0.1\n",
    "\n",
    "ww = optimize(X_train, y_train, lam, alpha)\n",
    "print(ww)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hh_6i_s18AgP"
   },
   "source": [
    "　`optimize()`関数は、重みの変化を全て記録しているので、変化を描画することが可能である。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "12JJMmdmxdhE"
   },
   "outputs": [],
   "source": [
    "plt.plot(ww[:,0], ww[:,1], \"o-\", linewidth=0.5, markersize=3) # 丸マーカー付きの折れ線グラフ o-\n",
    "plt.xlabel('$w_1$')\n",
    "plt.ylabel('$w_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XxklqUHTyCBK"
   },
   "source": [
    "　右下の $w=(0,0)$ を始点として、左上方向に $w$ が更新され、最終的にscikit-learnの結果と同じ $\\hat w = (-0.92, 1.46)$ が得られた。また、最終的な解に近づくにつれ更新幅が減少していることもわかる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o2CMbUC79y0s"
   },
   "source": [
    "------\n",
    "##### 課題 AI4.5\n",
    "\n",
    "　$\\alpha$ の値を変更させることで、学習の進み方はどうなるだろうか。$\\alpha = 0.001$ 、 $\\alpha = 10$ 、およびその間の値を試すことで、**$\\alpha$ が大きくなりすぎた時と $\\alpha$ が小さくなりすぎた時のそれぞれについてどういうことが起きるか簡潔に述べよ。**\n",
    "\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "37DMY818jzZy"
   },
   "source": [
    "# レポート提出について\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rLzUiGyLXOdo"
   },
   "source": [
    "## レポートの提出方法\n",
    "　レポートは**答案テンプレートを用い**、**1つのファイル (.doc, .docx, .pdf)** にまとめ、**学籍番号と氏名を確認の上**、**2020/1/9 15:00 (次回 基盤人工知能演習) までに東工大ポータルのOCW-iから提出**すること。\n",
    "ファイルのアップロード後、OCW-iで「提出済」というアイコンが表示されていることを必ず確認すること。それ以外の場合は未提出扱いとなるので十分注意すること。\n",
    "また、締め切りを過ぎるとファイルの提出ができないため、時間に余裕を持って提出を行うこと。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2z6NkAX-3IQG"
   },
   "source": [
    "## 答案テンプレート\n",
    "\n",
    "```\n",
    "学籍番号:\n",
    "名前:\n",
    "\n",
    "課題 AI4.1\n",
    "1. 予測されたy：[__, __, __, __, __, __, __, __, __, __]\n",
    "2. 正解率：\n",
    "\n",
    "課題 AI4.3\n",
    "w_2 = _____ の時、lossの値が最小になる\n",
    "\n",
    "課題 AI4.4（発展）\n",
    "この損失関数は w_1 について下に凸になって{いる | いない}。\n",
    "\n",
    "課題 AI4.5\n",
    "αが大きくなりすぎると__________________\n",
    "αが小さくなりすぎると__________________\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AnkP8JOw3frB"
   },
   "source": [
    "# 補足資料\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XPDdQ9w839S3"
   },
   "source": [
    "## ※1 微分ができない場合の処理\n",
    "微分ができない時は微分の定義から周囲に$w$の値を僅かにずらして、微分の定義に従って値を推定する。ただし、これを行うためには最低でも変数の数だけ微小にずらした座標の値の計算を行うため、計算的にはかなりコストが大きい。"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AI4_Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
