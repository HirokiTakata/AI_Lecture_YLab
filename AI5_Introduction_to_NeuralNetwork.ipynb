{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1m2cM919WnWi"
   },
   "source": [
    "# 基盤人工知能演習 第5回\n",
    "\n",
    "※本演習資料の二次配布・再配布はお断り致します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hJeDusaem7Qw"
   },
   "source": [
    "　今回から3回かけて、ニューラルネットワーク (Neural network) を用いた分類問題を学んでいく。本日の内容は以下の通りである。\n",
    "\n",
    "- **`torch.tensor`を利用した単層パーセプトロン (Single layer perceptron) の実装**\n",
    "\n",
    "- **`torch.nn.Linear()`を利用した単層パーセプトロンの実装**\n",
    "\n",
    "- **GPU (Graphics Processing Unit) を用いた計算**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gjDKatrppQz6"
   },
   "source": [
    "## AI5.1 | 最急降下法のおさらい\n",
    "\n",
    "　前回（基盤人工知能演習 第4回）、ロジスティック回帰の内部実装で「最急降下法 (steepest gradient descent)」に基づいて重みを最適化したことを覚えているだろうか。\n",
    "\n",
    "　**最急降下法はニューラルネットワークでも極めて重要**であるため、まずは最急降下法のおさらいをしてみる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "agfJlMjzqp0y"
   },
   "source": [
    "-----\n",
    "\n",
    "##### 課題 AI5.1\n",
    "\n",
    "　最急降下法は以下の式で重み $w$ を更新していくものだった。\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "w^{(t+1)} & = & w^{(t)} - \\eta \\frac{\\partial L(w)}{\\partial w}\n",
    "\\end{eqnarray}$\n",
    "\n",
    "（ニューラルネットワークの文脈に合わせるために、 $\\alpha$ を $\\eta$ に書き直している）\n",
    "\n",
    "　$w$ は1次元で $L(w) = w^2 + 2w$ であるとき、以下のコードの`__xxxxx__`, `__yyyyy__` を埋めて最急降下法を完成させよ。（ $w=-1$ に収束することを確認することで実装が正しいか推定すると良い）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "28Q4l06xqpN3"
   },
   "outputs": [],
   "source": [
    "# 勾配を計算する関数\n",
    "# 自分で dL(w)/dw の微分を行い、その結果に従って gradient() を実装せよ\n",
    "def gradient(w):\n",
    "  return __xxxxx__;\n",
    "\n",
    "def optimize(w_initial, eta=0.1, steps=100):\n",
    "  w_now = w_initial\n",
    "  for i in range(steps):\n",
    "    w_now -= __yyyyy__;\n",
    "  return w_now\n",
    "\n",
    "print(optimize(1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p3wDWM-ztrs9"
   },
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L-gPrKaY18Tk"
   },
   "source": [
    "## AI5.2 | データセットの作成（前回演習と同じデータセット）\n",
    "\n",
    "　本日は、ロジスティック回帰を学習した時と同じデータセットを作成して、これを題材にニューラルネットワークを学んでいくことにする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dw4JberG2nAe"
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L1nwXMbu2v96"
   },
   "outputs": [],
   "source": [
    "# 仮想的なデータの作成\n",
    "np.random.seed(0)\n",
    "n = 40\n",
    "X_train = np.random.randn(n, 2)\n",
    "\n",
    "noise = 1.6 * np.random.randn(n)                        # 結果にノイズが含まれている\n",
    "y_train = - X_train[:,0] + 2 * X_train[:,1] + noise > 0 # -x1 + 2 x2 > 0 をpositiveとして定義する\n",
    "\n",
    "# 観測されたデータを散布図で示す\n",
    "plt.scatter(X_train[:,0][y_train==True],X_train[:,1][y_train==True],  \n",
    "            marker=\"o\", label=\"positives\")           # positive (y =  True) を o で表示\n",
    "plt.scatter(X_train[:,0][y_train==False],X_train[:,1][y_train==False], \n",
    "            marker=\"x\", label=\"negatives\")           # negative (y = False) を x で表示\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fYfNzyR5FEop"
   },
   "source": [
    "## AI5.3 | PyTorchの利用\n",
    "\n",
    "　この演習においては、ニューラルネットワークを構築するためのPythonライブラリとして、PyTorchを利用する（**補足資料 ※1**）。ニューラルネットワークのライブラリはバージョンが頻繁に更新されるため、再現性の担保のためにも常にバージョンを確認しておくと良い。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ef-hlG7AG_XN"
   },
   "outputs": [],
   "source": [
    "# Google Colabでは、最初からPyTorchがインストールされている\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZBjcPa-aHy0t"
   },
   "outputs": [],
   "source": [
    "# ライブラリのバージョン確認\n",
    "print(torch.__version__)\n",
    "# -> 1.3.1（19.12.20 現在）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5dbAlkLAG3yF"
   },
   "source": [
    "### AI5.3.1 | torch.tensorの利用\n",
    "\n",
    "　torchの最も重要なものが`torch.tensor`である。\n",
    "最も単純に説明すると、**NumPy配列`np.array`がさらに賢くなったものが`torch.tensor`**である。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IETibDt0JS1H"
   },
   "outputs": [],
   "source": [
    "A = torch.tensor([[1,2], [3,4]])\n",
    "print(A)\n",
    "print(A.dim())\n",
    "print(A.shape) \n",
    "print(A[0,0]) # 1つの要素もtensorで表現される"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IKnZhllFKKwV"
   },
   "source": [
    "　`torch.tensor`が最も効果を発揮するのは、勾配 (gradient) の計算である。以下のコードを実行して、$y = wx$ の $(x, w) = (1, 2)$ における勾配 $\\frac{\\partial y}{\\partial x}, \\frac{\\partial y}{\\partial w}$を計算してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t9lBOuVDJmpb"
   },
   "outputs": [],
   "source": [
    "# requires_grad=Trueで勾配計算の対象を指定\n",
    "# 入力値は必ず実数値でなければならない\n",
    "# 整数値を入力するとエラーが発生する\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "y = w * x\n",
    "print(y)\n",
    "\n",
    "# 勾配を計算\n",
    "# ニューラルネットワークにおけるbackpropagation（誤差逆伝播）で利用するので\n",
    "# 逆方向、という意味でbackwardというメソッド名になっている\n",
    "y.backward()\n",
    "\n",
    "# x, wに関する勾配を出力する\n",
    "print(\"dy/dx (w=2) =\", x.grad)\n",
    "print(\"dy/dw (x=1) =\", w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oCpwFThHLMKz"
   },
   "source": [
    "　これだけのコードで勾配の値を求めることができた。もう少し複雑な場合も実行してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jr-z1hNtOwjb"
   },
   "outputs": [],
   "source": [
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "y = torch.exp(x*x)\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o9Box19WPdBA"
   },
   "source": [
    "　$y = e^{x^2}$ の時、 $\\frac{d y}{d x} = 2xe^{x^2}$ なので、x=1の時の値は $2e = 5.43656...$ である。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CrnbFjVmn-ld"
   },
   "source": [
    "　また、行列計算に対しても、勾配を求めることができる。\n",
    "以下の例は、行列 $A$ と行列 $B$ の行列積のトレース $tr(AB)$ について、$A$ の勾配を求めている。（これは $\\frac{\\partial tr(AB)}{\\partial A} = B^T$ となる）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t1EYrldVoEkx"
   },
   "outputs": [],
   "source": [
    "mat_A = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
    "mat_B = torch.tensor([[5.0, 6.0], [7.0, 8.0]])\n",
    "\n",
    "mat_C = mat_A.mm(mat_B) # mm : matrixとmatrixの行列積を計算する\n",
    "tr = mat_C.trace()\n",
    "tr.backward() # 対角成分の和に関する勾配を計算\n",
    "print(mat_A.grad) #mat_Aの勾配"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lwU0zgNOn9t_"
   },
   "source": [
    "　ちなみに、**`torch.tensor`に対しては、NumPyの関数は使わず、PyTorchの関数を利用しなければならない**。そうしないと、勾配が計算できなくなってしまうからだ（エラー表示としては、`torch.tensor`を`numpy.array`に直してからNumPy関数を使え、というメッセージになるので注意）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jJ1su0R3QZvd"
   },
   "outputs": [],
   "source": [
    "# ダメな例：RuntimeErrorが発生する\n",
    "import numpy as np\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "y = np.exp(x*x)\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "07k9q4UJSSJk"
   },
   "source": [
    "------\n",
    "##### 課題 AI5.2\n",
    "\n",
    "　シグモイド関数 $\\sigma(a) = \\frac{1}{1+e^{-a}}$ について、 $a=1$ の勾配 $\\frac{d \\sigma(a)}{d a}$ を`torch.tensor`を利用して求めよ。**ただし、`a.sigmoid()`は用いないこと**。\n",
    "\n",
    "　課題提出時には、以下のコードの`__xxxxx__`部分を答えよ。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cy5phPyGTcRD"
   },
   "outputs": [],
   "source": [
    "a = torch.tensor(1.0, requires_grad=True)\n",
    "sigma = __xxxxx__\n",
    "sigma.backward()\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HBUje2tntRPA"
   },
   "outputs": [],
   "source": [
    "## 以下のコードと同じ結果が得られるべき\n",
    "a = torch.tensor(1.0, requires_grad=True)\n",
    "sigma = a.sigmoid()\n",
    "sigma.backward()\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cuk0n-fGUVyj"
   },
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t-e-tzAbRBbO"
   },
   "source": [
    "### AI5.3.2 | `torch.tensor`を利用した単層パーセプトロンの実装\n",
    "\n",
    "　それでは、これまで使い方を簡単に見てきた`torch.tensor`を利用して、単層パーセプトロン (Single layer perceptron; SLP) の実装を行ってみる。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8IqA6htpRgSl"
   },
   "source": [
    "#### データセットの整形\n",
    "\n",
    "　PyTorchでは、学習に用いる入力 $X$ および出力 $y$ について、以下の条件を満たす必要がある。\n",
    "\n",
    "* 入力・出力はいずれも`torch.tensor`で表現されている\n",
    "* 入力・出力はいずれも 行列である\n",
    "\n",
    "特に、**出力の形式はscikit-learnの時と異なって行列**である必要があることに注意しよう。この条件を満たすように、5.2節で作成したデータ`train_X, train_y`をPyTorch用に変換する。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VEcVqEvFbQSK"
   },
   "outputs": [],
   "source": [
    "###### データの準備 ######\n",
    "\n",
    "# yをベクトルから行列にする\n",
    "Y_train = y_train[:, np.newaxis]\n",
    "\n",
    "# NumPy配列をPyTorch用のデータ形式に変換する\n",
    "# 学習のために、X, Yは実数値で表現する（dtype=torch.float）\n",
    "# また、X, Yは学習対象ではないので requires_grad=Trueは行わない\n",
    "X_torch = torch.tensor(X_train, dtype=torch.float)\n",
    "Y_torch = torch.tensor(Y_train, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AD-S_9eIKGUb"
   },
   "source": [
    "　さらに、DatasetとDataLoaderというものを作る。これは、XとYの組を維持してくれるPyTorch特有の便利な仕組みである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2jiSSKysKFYi"
   },
   "outputs": [],
   "source": [
    "# XとYの組み合わせを保持してくれる便利なモノ\n",
    "dataset = torch.utils.data.TensorDataset(X_torch, Y_torch)\n",
    "# XとYの組み合わせを(batch_size)個ずつ出力する便利なモノ\n",
    "# shuffle=True: 学習のたびにデータの順番を替えることで、データの並びに学習結果が影響されにくくする\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7cjrYbtEtKLk"
   },
   "outputs": [],
   "source": [
    "## dataloaderはfor loopで配列のように使える\n",
    "## shuffle=Trueの場合、実行するたびにデータの順番が変化する\n",
    "for X, Y in dataloader:\n",
    "  print(\"======\")\n",
    "  print(\"X is\", X)\n",
    "  print(\"Y is\", Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gPnu6-UgktMf"
   },
   "source": [
    "#### 重み $w$ の定義\n",
    "\n",
    "　そうしたら、次は単層パーセプトロンの重みを表現する $w$ を定義する。今回の場合は<font color=\"red\">2</font>つの特徴量から<font color=\"blue\">1</font>つの値を予測するので、 $w$ は <font color=\"red\">2</font>×<font color=\"blue\">1</font> の行列で表現されるべきだ（行列なので、コードでは大文字の`W`で表現する）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7l0G1Bh1emsm"
   },
   "outputs": [],
   "source": [
    "# W の定義（乱数で初期化する）\n",
    "torch.manual_seed(0) # 毎回同じ実行結果が得られるように乱数を固定する\n",
    "W = torch.randn(2, 1, dtype=torch.float, requires_grad=True)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pXdaKkS7fkBC"
   },
   "source": [
    "#### 学習のコードの作成\n",
    "\n",
    "　次に、学習のコードを記述する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6uz-C7txlMsZ"
   },
   "source": [
    "　コードを書くために、まず学習の流れを確認する（複雑なコードを書く場合は、このようにまず日本語で流れを整理すると、見通しが良くなる）。\n",
    "\n",
    "1. 以下2.-6.をepoch数だけ繰り返す\n",
    "2. 以下3.-6.をデータの個数だけ繰り返す\n",
    "3. あるデータ $x$ について、現在の重み $w$ を使って、$y = 1$ の確率 $p(y=1|x, w)$ を計算する。\n",
    "4. 実際の $y$ と予測結果を比較し、損失を計算する。\n",
    "5. 損失を元に、誤差逆伝播法 (backpropagation) で勾配 $\\frac{\\partial L(w)}{\\partial w}$を計算する。\n",
    "6. 学習率 $\\eta$ を使って、最急降下法と同一の更新式 $w^{(t+1)} = w^{(t)} - \\eta \\frac{\\partial L(w)}{\\partial w}$ で重み $w$ を更新する。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hjm5MFUqI1JP"
   },
   "source": [
    "　○×の2値分類の場合には**シグモイド関数 (sigmoid function, $\\sigma(a)$) を用いることで、予測値を0~1の確率値に変換**することができるので、この結果をもとに 3. $p(y=1|x,w)$ の計算を行う。\n",
    "\n",
    "　また、4. の損失については、**分類予測の損失関数（誤差関数）は交差エントロピー誤差 (cross entropy loss, $H(y, \\hat{y})$)** が一般的に使われる。\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "    \\sigma(a) & = & \\frac{1}{1+e^{-a}} \\\\\n",
    "H(y, \\hat{y}) & = & -y\\log(\\hat{y}) -(1-y)\\log(1-\\hat{y})\n",
    "\\end{eqnarray}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KTrcNpFV76NE"
   },
   "source": [
    "　上記の学習の流れを **図 AI5.1** に、誤差逆伝播法の部分を **図 AI5.2** に示す。\n",
    "\n",
    "![図 AI5.1](https://i.imgur.com/GfcPUMr.png)\n",
    "\n",
    "**図 AI5.1 | 学習の大まかな流れ** `DataLoader(X, y, batch_size=1, shuffle=True)` 、データが3件しかない場合を例として示している。前述の箇条書き1-6を全て含めている。\n",
    "\n",
    "![図 AI5.2](https://i.imgur.com/VsfdLcp.png)\n",
    "\n",
    "**図 AI5.2 | 誤差逆伝播** 右から順番に（逆方向に）、局所の偏微分値を積算することで、勾配 $\\frac{\\partial l}{\\partial w_1}, \\frac{\\partial l}{\\partial w_2}$ を計算する。 $\\Sigma, \\sigma, H$ はそれぞれ総和、シグモイド関数、交差エントロピーを意味する。\n",
    "\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F4xMv4v_KjJ0"
   },
   "source": [
    "　以上の情報を全てコードにまとめると、以下のコードが出来上がる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0qjtFEMEfur9"
   },
   "outputs": [],
   "source": [
    "# 複数のデータXを入力としてそれぞれのデータが正例である確率 y_pred_proba を返す関数\n",
    "# input: 行列 X, 行列 W\n",
    "# output: 1次元配列 y_pred_proba\n",
    "def predict_proba(X, W):\n",
    "  y_pred_proba = torch.mm(X, W).sigmoid()\n",
    "  return y_pred_proba\n",
    "\n",
    "# 交差エントロピー誤差を計算\n",
    "def cross_entropy(y_pred, y_true):\n",
    "  ret = 0\n",
    "  ret -=    y_true  * torch.log(    y_pred) # for y=1\n",
    "  ret -= (1-y_true) * torch.log(1 - y_pred) # for y=0\n",
    "  return ret\n",
    "\n",
    "def train(X, Y, W_original, eta=0.01, epoch = 100):\n",
    "  torch.manual_seed(0) ## 毎回同じ結果になるようにする\n",
    "  # 入力として与えたWそのものを書き換えないようにするおまじない\n",
    "  W = W_original.clone().detach().requires_grad_(True)\n",
    "\n",
    "  # 1. epoch数だけ繰り返す\n",
    "  for t in range(epoch):\n",
    "    # 2. データの個数だけ繰り返す\n",
    "    for data_X, data_Y in dataloader: # dataloaderからX, Yを同時に取り出す\n",
    "      # 3. p(y=1|x,w)を求める\n",
    "      y_proba = predict_proba(data_X, W)\n",
    "      # 4. 損失の計算\n",
    "      loss = cross_entropy(y_proba, data_Y)\n",
    "      # 5. 誤差逆伝播法の実施（図AI5.2）\n",
    "      # これによってW.gradが計算される\n",
    "      loss.backward()\n",
    "\n",
    "      # 勾配計算を行わないように一時的に（with構文の範囲だけ）制限する\n",
    "      with torch.no_grad(): \n",
    "        # 6. 計算された勾配と学習率 eta を使って重みを更新\n",
    "        W -= eta * W.grad \n",
    "        W.grad.zero_() # W.gradの中身を0に戻す\n",
    "  return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n815a7YAN01H"
   },
   "source": [
    "　ところで、このように**一部のデータの損失から重み $w$ を繰り返し更新する方法を確率的勾配降下法 (stochastic gradient descent; SGD)** と呼ぶ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Qnj2bM0ubxV"
   },
   "source": [
    "#### 学習の実行と学習結果の確認\n",
    "\n",
    "　それでは、先ほど作成した `train()` 関数を用いて学習を実行してみるのだが、その前に、未学習の時点での予測正解率を確認してみよう。\n",
    "\n",
    "　データの予測は、以下のコードで行うことができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4hXuwbS08JPR"
   },
   "outputs": [],
   "source": [
    "count_ok  = 0\n",
    "count_all = 0\n",
    "\n",
    "for data_X, data_Y in dataloader:\n",
    "  y_pred_proba = predict_proba(data_X, W) # W は学習前の重み\n",
    "  y_pred = y_pred_proba > 0.5\n",
    "  result = y_pred == data_Y\n",
    "  count_ok += result.numpy()[0,0] # true -> 1, false -> 0 となる\n",
    "print(count_ok, \"/\", len(Y_torch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AOvSwSrN8Vfq"
   },
   "source": [
    "　柳澤がやった環境では、14/40件のみが予測に成功した。○か×かを予測するだけなので、ランダムにあてずっぽうを言うよりも精度が悪い状況だ。\n",
    "\n",
    "　それでは、気を取り直して学習を行う。これまで書いてきた関数 `train()` を利用することで、簡単に学習が可能なはずである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Ymu4GeCudwe"
   },
   "outputs": [],
   "source": [
    "W_hat = train(X_torch, Y_torch, W)\n",
    "print(W_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8SxM0ZCyvZlB"
   },
   "source": [
    "　学習が行われ、`W_hat`が作成された。\n",
    "これを元に、学習で利用したデータを予測してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "upBzppMEwgIn"
   },
   "outputs": [],
   "source": [
    "count_ok  = 0\n",
    "count_all = 0\n",
    "\n",
    "for data_X, data_Y in dataloader:\n",
    "  y_pred_proba = predict_proba(data_X, W_hat)\n",
    "  y_pred = y_pred_proba > 0.5\n",
    "  result = y_pred == data_Y\n",
    "  count_ok += result.numpy()[0,0] # true -> 1, false -> 0 となる\n",
    "print(count_ok, \"/\", len(Y_torch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xLw9O0mhzujh"
   },
   "source": [
    "　40個のデータのうち、31個は正しく予測ができるようになった。ちゃんと学習できているようだ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "or4XJTylAtNB"
   },
   "source": [
    "------\n",
    "\n",
    "##### 課題 AI5.3\n",
    "\n",
    "　$x_{new} = (1,0)$ という新しい入力を与えた時の、`y_proba`の値を計算せよ。また、その結果から、この $x$ はどちらのクラスに属すると予想されるか答えよ。（ヒント：`torch.tensor`を準備する際に、`dtype=torch.float`を忘れないようにせよ）\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wb2GdZfrNxTF"
   },
   "source": [
    "### AI5.3.3 | PyTorchに準備されている「層」を利用した単層パーセプトロンの実装\n",
    "\n",
    "　以上のコードは講義の内容の理解を深めるために行ってきた。しかし、実際には、PyTorchは様々なネットワーク構造や損失関数、重み $w$ の最適化手法を簡単に使うことができる。\n",
    "\n",
    "　シグモイド関数と交差エントロピー誤差による単層パーセプトロンは以下のように実装できる（先ほどと違い、**図 AI5.2** の右図のように定数項  `bias` が含まれていることに注意してほしい）。\n",
    "\n",
    "![図 AI5.2](https://i.imgur.com/De4HsRh.png)\n",
    "\n",
    "**図 AI5.2 | 単層パーセプトロンにおける定数項（bias項）の有無**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dcnRcy7xH-Hb"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QGsa_D6XXtre"
   },
   "outputs": [],
   "source": [
    "###### 単層パーセプトロン (Single Layer Perceptron; SLP) の定義 ######\n",
    "torch.manual_seed(0) # 同じ結果が出るようにする\n",
    "slp = torch.nn.Sequential(\n",
    "  torch.nn.Linear(2, 1, bias=True),   # Linearの中で重み w が定義され、それが更新される\n",
    ")\n",
    "\n",
    "# torch.nn.Linear(2, 1) だけでもbias=Trueになる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V6cg-cJiJ_vh"
   },
   "outputs": [],
   "source": [
    "###### 重み w の更新に関する設定 ######\n",
    "\n",
    "# 交差エントロピー誤差を計算する関数\n",
    "# Binary Cross Entropy がBCEと略されている\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 確率的勾配降下法 (SGD) によるwの最適化\n",
    "# lr（learning rate、学習率）がetaに対応する\n",
    "optimizer = torch.optim.SGD(slp.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uqbaTYQOAUB0"
   },
   "source": [
    "　AI5.3.2節では重み `W` を明示的に定義したが、PyTorchの実装では **`torch.nn.Linear` の中に `W` に対応する `weight` や `bias` が存在している**。初期状態ではどのような値になっているか、確認してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xgf4ih0AA9Aq"
   },
   "outputs": [],
   "source": [
    "print(slp.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5JXgx0JZBEAL"
   },
   "source": [
    "`0.weight`などと書かれているが、これは、0層目（一番最初の層）の重み $w$ という意味である。また、`0.bias`とは一番最初の層のバイアス項である。この層は $y = w^t \\cdot x + b$ を計算していることになる。\n",
    "\n",
    "　それでは、次に示すコードを用いて学習を行ってみる。 AI5.3.2節では `W -= eta * W.grad` によって `W` を更新したが、PyTorchのoptimizerを用いる場合には **`optimizer.step()` を実行することで `W` に対応する `weight` や `bias` が更新される**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-cS9u2D91Xy_"
   },
   "outputs": [],
   "source": [
    "###### 学習の実行 #######\n",
    "\n",
    "nEpoch = 50\n",
    "for t in range(nEpoch):\n",
    "  for data_X, data_Y in dataloader:\n",
    "    Y_pred_proba = slp(data_X)           # 予測を実施\n",
    "    loss = loss_fn(Y_pred_proba, data_Y) # 損失関数を計算\n",
    "    #print(t, loss.item())\n",
    "\n",
    "    optimizer.zero_grad() # 傾きの計算をするための初期化\n",
    "    loss.backward()       # 各wについて、誤差逆伝播法で dL(w)/dw を計算\n",
    "    optimizer.step()      # SGDの更新式に従いwを更新\n",
    "\n",
    "  print(t, slp.state_dict())   # 学習の進行を確認するためにweightとbiasを出力してみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TwmuvRZzDKb6"
   },
   "source": [
    "　これで学習を行うことができた。この学習済みの `slp` を用いて、予測を行ってみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n51Y2eUtShSx"
   },
   "outputs": [],
   "source": [
    "###### 予測精度の計算 ######\n",
    "\n",
    "# 複数の予測値と複数の正解値から予測精度を計算する\n",
    "# True -> 1, False -> 0 と処理されることを利用すると\n",
    "# mean()で予測正解率を計算できる\n",
    "def accuracy(Y_pred, Y_true):\n",
    "  result = (Y_pred == Y_true)\n",
    "  return result.numpy().mean()\n",
    "\n",
    "Y_pred_proba = slp(X_torch).sigmoid() # 予測の実施（sigmoid()によって確率を計算）\n",
    "Y_pred = Y_pred_proba > 0.5           # 確率が0.5以上かそれ未満かで○×を予測\n",
    "print(Y_pred_proba)\n",
    "print(accuracy(Y_pred, Y_torch))      # 予測精度を計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zs7uXHwgcF1g"
   },
   "source": [
    "　これによりそれぞれのデータ `X` に対する正例である確率を計算し、その確率にしたがって〇/×の予測を行うことができた。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oITz-VFqfG0M"
   },
   "source": [
    "　ところで、ニューラルネットワークの学習では、1つのデータごとに更新するのではなく、**複数のデータごとに $w$ の勾配の平均を計算し、更新を行うミニバッチ (mini-batch)** という仕組みが通常用いられる。一度に学習するデータ数を示す `batch_size` を大きくすることで、**計算速度を向上させ**、**異常な値を持つデータの影響を比較的少なくする**ことができる。\n",
    "\n",
    "　先ほど実行したコードで、DataLoaderの定義の部分に `batch_size=1` という記述があるが、これを `batch_size=4` とすると、4つのデータに基づいて $w$ を更新するようになる。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WFd4se-Ete3b"
   },
   "outputs": [],
   "source": [
    "### batch_size = 4に変更してみる\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y13Vofh7EP6Y"
   },
   "outputs": [],
   "source": [
    "## dataloaderの出力を再度確認してみる\n",
    "\n",
    "## dataloaderはfor loopで配列のように使える\n",
    "## shuffle=Trueの場合、実行するたびにデータの順番が変化する\n",
    "for X, Y in dataloader:\n",
    "  print(\"======\")\n",
    "  print(\"X is\", X)\n",
    "  print(\"Y is\", Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ESSbEjJQEb2n"
   },
   "source": [
    "　たしかに、dataloaderは4つのデータを同時に出力していることが確認できた。それでは、先ほどと同様に学習を行ってみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P7oNXZmJXqWD"
   },
   "outputs": [],
   "source": [
    "###### 単層パーセプトロン (Single Layer Perceptron; SLP) の定義 ######\n",
    "torch.manual_seed(0) # 何度実行しても同じ結果が出るようにする\n",
    "slp = torch.nn.Sequential(\n",
    "  torch.nn.Linear(2, 1, bias=True), \n",
    ")\n",
    "\n",
    "###### 重みの更新に関する設定 ######\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(slp.parameters(), lr=0.1) # slpのwを更新することを指定する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-FB8f-q31hiZ"
   },
   "outputs": [],
   "source": [
    "###### 学習の実行 #######\n",
    "nEpoch = 50\n",
    "for t in range(nEpoch):\n",
    "  for data_X, data_Y in dataloader:\n",
    "    Y_pred_proba = slp(data_X)           # 予測を実施\n",
    "    loss = loss_fn(Y_pred_proba, data_Y) # 損失関数を計算\n",
    "    #print(t, loss.item())\n",
    "\n",
    "    optimizer.zero_grad() # 傾きの計算をするための初期化\n",
    "    loss.backward()       # 各wについて、誤差逆伝播法で dL(w)/dw を計算\n",
    "    optimizer.step()      # SGDの更新式に従いwを更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ROqdotnEtkON"
   },
   "outputs": [],
   "source": [
    "Y_pred_proba = slp(X_torch).sigmoid() # 予測の実施\n",
    "Y_pred = Y_pred_proba > 0.5           # 確率が0.5以上かそれ未満かで○×を予測\n",
    "print(accuracy(Y_pred, Y_torch))      # 予測精度を計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SdgjdPxOtsUa"
   },
   "source": [
    "　バッチサイズは $2^n$ を使うことが慣例的に多く、データ数が万単位で存在するのであれば128, 256, 512、場合によっては1024以上の値にする。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E1ut90qwvo_d"
   },
   "source": [
    "　最後に、この単層パーセプトロンのモデル `slp` の予測結果が、どのように $x_1, x_2$ に依存しているか見てみよう。以下の `draw()` は$x_1, x_2$の空間に予測された正例（〇）確率を等高線で表示する関数になっている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tGBMR4nZ2t1n"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def draw(aModel, X, y):\n",
    "  # `aModel` を用いてグラデーションを描画する\n",
    "  xx1 = np.arange(-3,3,0.1)\n",
    "  xx2 = np.arange(-3,3,0.1)\n",
    "  XX1,XX2 = np.meshgrid(xx1, xx2)\n",
    "  XX = torch.tensor(\n",
    "      np.hstack([XX1.reshape(-1,1),\n",
    "                 XX2.reshape(-1,1)]),\n",
    "      dtype=torch.float\n",
    "  )\n",
    "  YY = aModel(XX).sigmoid().detach().numpy().reshape(*XX1.shape)\n",
    "  plt.contourf(XX1, XX2, YY, levels=[0.0,0.25,0.5,0.75,1.00])\n",
    "  plt.colorbar()\n",
    "\n",
    "  # 学習に用いたデータ点のプロット\n",
    "  plt.scatter(X[:,0][y==True],  X[:,1][y==True],  marker=\"o\", label=\"positives\") \n",
    "  plt.scatter(X[:,0][y==False], X[:,1][y==False], marker=\"x\", label=\"negatives\") \n",
    "\n",
    "  plt.xlabel('$x_1$')\n",
    "  plt.ylabel('$x_2$')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7FHR8LgHdygT"
   },
   "outputs": [],
   "source": [
    "draw(slp, X_train, y_train) # 単層パーセプトロンの学習結果を描画"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b5xodAIKxNtL"
   },
   "source": [
    "　結果はどうなったであろうか。直線的な等高線が描画されたはずである。このように単層パーセプトロンは、基盤人工知能演習 第4回で学習したロジスティック回帰と同様に**線形モデル**である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zjTo41KH68Nx"
   },
   "source": [
    "--------\n",
    "##### 課題 AI5.4\n",
    "\n",
    "　次回の基盤人工知能では、多層パーセプトロン (Multi layer perceptron) を学ぶのだが、先にモデルを構築してみよう。\n",
    "\n",
    "　以下に示す多層パーセプトロンモデルについて、学習を行い、同様に等高線を描き、等高線が直線であるか曲線であるかを答えよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xXL2MQvU7y30"
   },
   "outputs": [],
   "source": [
    "# 多層パーセプトロン (Multi Layer Perceptron; MLP)\n",
    "# 間にSigmoid()を入れることを忘れずに\n",
    "torch.manual_seed(0) # 結果を固定させる\n",
    "\n",
    "mlp = torch.nn.Sequential(\n",
    "  torch.nn.Linear(2, 2),   # 2変数→2変数\n",
    "  torch.nn.Sigmoid(),\n",
    "  torch.nn.Linear(2, 1),   # 2変数→1変数\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(mlp.parameters(), lr=0.1) # ここを mlp.paramters()に変更することを忘れずに"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V1UNC3mgxpIT"
   },
   "outputs": [],
   "source": [
    "###### 学習の実行 ######\n",
    "nEpoch = 250 # epoch数を増やした\n",
    "for t in range(nEpoch):\n",
    "  for data_X, data_Y in dataloader:\n",
    "\n",
    "    # ここに学習のコードを実装せよ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jz7zQXyYxufj"
   },
   "outputs": [],
   "source": [
    "###### 等高線の描画 ######\n",
    "draw(mlp, X_train, y_train) # 多層パーセプトロンの学習結果を描画"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kv9qfdjsY6xC"
   },
   "source": [
    "##### 課題 AI5.5（発展）\n",
    "\n",
    "　中間層に`Sigmoid()`を入れ忘れてしまった多層パーセプトロンの学習を行った時の等高線を確認せよ。直線的だろうか曲線だろうか。なぜこのようになるのか、考察せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sswo3Lc3Y5t8"
   },
   "outputs": [],
   "source": [
    "# sigmoidを入れ忘れた多層パーセプトロン\n",
    "torch.manual_seed(0) # 結果を固定させる\n",
    "\n",
    "mlp_wo_sigmoid = torch.nn.Sequential(\n",
    "  torch.nn.Linear(2, 2),   # 2変数→2変数\n",
    "  torch.nn.Linear(2, 1),   # 2変数→1変数\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x9mNuKcH8OyS"
   },
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o_hxtCAoJfAc"
   },
   "source": [
    "## AI5.4 | GPU (Graphic Processing Unit) を用いた計算\n",
    "\n",
    "　昨今のニューラルネットワークの隆盛に一役買っているのが、**GPU (Graphic Processing Unit) という部品**である。 この部品は Graphic の名前の通り、本来は**画面描画用のパーツ**である。\n",
    "\n",
    "![GPUの例](https://i.imgur.com/J3toOw5.jpg)\n",
    "\n",
    "このパーツを天体や流体、タンパク質などのシミュレーションなど、描画以外に転用することで、計算を高速化することができる。このGPUを使うことで、**ニューラルネットワークの計算の（数倍から10倍以上の）高速化**が達成できる。\n",
    "Google Colaboratoryでは、GPUを無料で使うことができる（**※2**）ので、これを用いた計算を行ってみる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1G4P-YFONxxj"
   },
   "source": [
    "　まず、**GPUがあるとしたらどこに存在するのか**を知る必要がある。これは `torch.device()` 関数で変数に入れることができる（実際にGPUがあるかどうかは確認していないので注意。今回のGoogle Colaboratoryでは、GPUが存在することを前提として構わない）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pWgT1WyCPrn-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "###### GPUの存在を教える ######\n",
    "# cudaはGPU上で様々な計算するためのプラットフォーム（ライブラリみたいなもの）の名前\n",
    "device_GPU = torch.device(\"cuda:0\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rZMARq_EQhHr"
   },
   "source": [
    "　GPUの場所を特定したら、**単層パーセプトロンのモデル**と**`X, y`のデータ**を**GPUへ転送**しながら学習を行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pxR7nnplNqMA"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0) # 結果を固定させる\n",
    "\n",
    "###### モデル（単層パーセプトロン）の定義 ######\n",
    "slp = torch.nn.Sequential(\n",
    "  torch.nn.Linear(2, 1),   # w_1 x_1 + w_2 x_2 + b に対応、Linearの中の w が更新される\n",
    ")\n",
    "slp.to(device_GPU)  # GPUにモデルを転送\n",
    "\n",
    "###### 重みの更新に関する設定 ######\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(slp.parameters(), lr=0.1) # slpのwを更新することを指定する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tQRVECaZNtLR"
   },
   "outputs": [],
   "source": [
    "###### 学習の実行 #######\n",
    "\n",
    "# batch_size = 4\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# 学習は高速になる…はずなのだが？？\n",
    "nEpoch = 50\n",
    "for t in range(nEpoch):\n",
    "  for data_X, data_Y in dataloader:\n",
    "    data_X = data_X.to(device_GPU)        # GPUにデータを転送\n",
    "    data_Y = data_Y.to(device_GPU)        # GPUにデータを転送\n",
    "    Y_pred_proba = slp(data_X)\n",
    "    loss = loss_fn(Y_pred_proba, data_Y)  \n",
    "    #print(t, loss.item())\n",
    "\n",
    "    optimizer.zero_grad() # 傾きの計算をするための初期化\n",
    "    loss.backward()       # 各wについて、誤差逆伝播法で dL(w)/dw を計算\n",
    "    optimizer.step()      # SGDの更新式に従いwを更新"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lKFgNZUmTNZf"
   },
   "source": [
    "　これで、GPUを使った学習が行われた。次に予測を行うが、予測を行う場合は次の手順を踏む。\n",
    "\n",
    "- `X` をGPUに転送する\n",
    "- GPU上で予測を行う\n",
    "- 予測結果をGPUから計算機に転送する\n",
    "\n",
    "最後の「GPUから計算機に転送」は `cpu()` を用いて実行することができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kTkSzVYZNvMS"
   },
   "outputs": [],
   "source": [
    "X_torch = X_torch.to(device_GPU)      # 予測時はXだけGPUに転送\n",
    "Y_pred_proba = slp(X_torch).sigmoid() # 予測の実施\n",
    "Y_pred_proba = Y_pred_proba.cpu()     # 予測結果をGPUから計算機に転送\n",
    "Y_pred = Y_pred_proba > 0.5           # 確率が0.5以上かそれ未満かで○×を予測\n",
    "print(accuracy(Y_pred, Y_torch))      # 予測精度を計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zc5_yYLIUN1L"
   },
   "source": [
    "　これで、GPUを使った学習と予測を実施できるようになった。**`to(device_GPU)`で計算機→GPUの転送、`cpu()`でGPU→計算機の転送**と覚えておこう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WwlGl4hyVNpV"
   },
   "source": [
    "　…ところで、学習速度はどのように変わっただろうか。GPUを使ってどの程度高速化されただろうか。実は `batch_site=4` である今回のケースだと、バッチサイズが小さく、GPUを効果的に使うことができない。\n",
    "次回以降はMNISTと呼ばれる手書き文字画像の分類を行うが、その時にはGPUを使わないと話にならないくらい遅くなってしまうので、先もって教えておいた、と理解してほしい。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "37DMY818jzZy"
   },
   "source": [
    "# レポート提出について\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rLzUiGyLXOdo"
   },
   "source": [
    "## レポートの提出方法\n",
    "　レポートは**答案テンプレートを用い**、**1つのファイル (.doc, .docx, .pdf)** にまとめ、**学籍番号と氏名を確認の上**、**1/16 15:00 (次回 基盤人工知能演習) までに東工大ポータルのOCW-iから提出**すること。\n",
    "ファイルのアップロード後、OCW-iで「提出済」というアイコンが表示されていることを必ず確認すること。それ以外の場合は未提出扱いとなるので十分注意すること。\n",
    "また、締め切りを過ぎるとファイルの提出ができないため、時間に余裕を持って提出を行うこと。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2z6NkAX-3IQG"
   },
   "source": [
    "## 答案テンプレート\n",
    "\n",
    "```\n",
    "学籍番号:\n",
    "名前:\n",
    "\n",
    "課題 AI5.1\n",
    "__xxxxx__ = \n",
    "__yyyyy__ = \n",
    "\n",
    "課題 AI5.2\n",
    "__xxxxx__ = \n",
    "\n",
    "課題 AI5.3\n",
    "y_proba = \n",
    "x_new は { 正例 | 負例 } に属すると予想される。\n",
    "\n",
    "課題 AI5.4\n",
    "多層パーセプトロンの場合、等高線は { 直線 | 曲線 } を描く。\n",
    "\n",
    "課題 AI5.5（発展）\n",
    "等高線は { 直線 | 曲線 } を描く。\n",
    "（考察を記述せよ）\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JRG94YFMVwGX"
   },
   "source": [
    "# 補足資料"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7qmXCLX-K4B"
   },
   "source": [
    "## ※1 | PyTorchのもうちょっと真面目な勉強資料\n",
    "\n",
    "　今回の講義では、時間の都合でいろんな知識を飛ばして教えている。\n",
    "より詳しく知りたい場合は、[PyTorchのtutorial](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) や [yunjeyによる非公式なtutorial](https://github.com/yunjey/pytorch-tutorial) を読み進めることをお勧めする。\n",
    "\n",
    "　日本語の書籍も存在する（例：「現場で使える！PyTorch開発入門―深層学習モデルの作成とアプリケーションへの実装」翔泳社）が、ニューラルネットワーク（深層学習）はライブラリの仕様などが頻繁に変更されているため、書籍が古いと最新版では存在しない関数を利用している可能性もある。PythonやPyTorchのバージョンには十分注意して、実際にGoogle Colaboratory等で動かしながら学習すると良い。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9FInQ2GYCpAm"
   },
   "source": [
    "## ※2 | Google ColaboratoryにおけるGPUの利用\n",
    "\n",
    "　今回授業資料として配布したJupyter Notebookは**GPU利用設定がすでにONになっており**、追加の設定なくGPUを使うことができる。一方、Google Colaboratoryの初期設定はGPU利用がOFFになっているので、ONにする方法を紹介する（以下の資料は[Chainer tutorialの「1.3.5. GPUを利用する」](https://tutorials.chainer.org/ja/01_Welcome_to_Chainer_Tutorial.html#GPU-%E3%82%92%E4%BD%BF%E7%94%A8%E3%81%99%E3%82%8B)から引用・一部修正している）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CeujeWtQDoBU"
   },
   "source": [
    "　まず、画面上部のタブの中の 「ランタイム」 (Runtime) をクリックし、「ランタイムのタイプを変更」 (Change runtime type) を選択する。\n",
    "\n",
    "　すると、「ノートブックの設定」 (Notebook Settings) というものが表示されるはずである。この中の「ハードウェア アクセラレータ」 (Hardware Accelerator)をNoneからGPUに変更して、保存すればよい。\n",
    "\n",
    "![GPUに変更](https://tutorials.chainer.org/ja/_images/01_08.png)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AI5_Introduction_to_NeuralNetwork.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
